article,abstract
"for about 20 years the problem of properties of short - term changes of solar activity has been considered extensively . many investigators studied the short - term periodicities of the various indices of solar activity . several periodicities were detected , but the periodicities about 155 days and from the interval of @xmath3 $ ] days ( @xmath4 $ ] years ) are mentioned most often . first of them was discovered by @xcite in the occurence rate of gamma - ray flares detected by the gamma - ray spectrometer aboard the _ solar maximum mission ( smm ) . this periodicity was confirmed for other solar flares data and for the same time period @xcite . it was also found in proton flares during solar cycles 19 and 20 @xcite , but it was not found in the solar flares data during solar cycles 22 @xcite . _ several autors confirmed above results for the daily sunspot area data . @xcite studied the sunspot data from 18741984 . she found the 155-day periodicity in data records from 31 years . this periodicity is always characteristic for one of the solar hemispheres ( the southern hemisphere for cycles 1215 and the northern hemisphere for cycles 1621 ) . moreover , it is only present during epochs of maximum activity ( in episodes of 13 years ) . similarinvestigationswerecarriedoutby + @xcite . they applied the same power spectrum method as lean , but the daily sunspot area data ( cycles 1221 ) were divided into 10 shorter time series . the periodicities were searched for the frequency interval 57115 nhz ( 100200 days ) and for each of 10 time series . the authors showed that the periodicity between 150160 days is statistically significant during all cycles from 16 to 21 . the considered peaks were remained unaltered after removing the 11-year cycle and applying the power spectrum analysis . @xcite used the wavelet technique for the daily sunspot areas between 1874 and 1993 . they determined the epochs of appearance of this periodicity and concluded that it presents around the maximum activity period in cycles 16 to 21 . moreover , the power of this periodicity started growing at cycle 19 , decreased in cycles 20 and 21 and disappered after cycle 21 . similaranalyseswerepresentedby + @xcite , but for sunspot number , solar wind plasma , interplanetary magnetic field and geomagnetic activity index @xmath5 . during 1964 - 2000 the sunspot number wavelet power of periods less than one year shows a cyclic evolution with the phase of the solar cycle.the 154-day period is prominent and its strenth is stronger around the 1982 - 1984 interval in almost all solar wind parameters . the existence of the 156-day periodicity in sunspot data were confirmed by @xcite . they considered the possible relation between the 475-day ( 1.3-year ) and 156-day periodicities . the 475-day ( 1.3-year ) periodicity was also detected in variations of the interplanetary magnetic field , geomagnetic activity helioseismic data and in the solar wind speed @xcite . @xcite concluded that the region of larger wavelet power shifts from 475-day ( 1.3-year ) period to 620-day ( 1.7-year ) period and then back to 475-day ( 1.3-year ) . the periodicities from the interval @xmath6 $ ] days ( @xmath4 $ ] years ) have been considered from 1968 . @xcite mentioned a 16.3-month ( 490-day ) periodicity in the sunspot numbers and in the geomagnetic data . @xcite analysed the occurrence rate of major flares during solar cycles 19 . they found a 18-month ( 540-day ) periodicity in flare rate of the norhern hemisphere . @xcite confirmed this result for the @xmath7 flare data for solar cycles 20 and 21 and found a peak in the power spectra near 510540 days . @xcite found a 17-month ( 510-day ) periodicity of sunspot groups and their areas from 1969 to 1986 . these authors concluded that the length of this period is variable and the reason of this periodicity is still not understood . @xcite and + @xcite obtained statistically significant peaks of power at around 158 days for daily sunspot data from 1923 - 1933 ( cycle 16 ) . in this paper the problem of the existence of this periodicity for sunspot data from cycle 16 is considered . the daily sunspot areas , the mean sunspot areas per carrington rotation , the monthly sunspot numbers and their fluctuations , which are obtained after removing the 11-year cycle are analysed . in section 2 the properties of the power spectrum methods are described . in section 3 a new approach to the problem of aliases in the power spectrum analysis is presented . in section 4 numerical results of the new method of the diagnosis of an echo - effect for sunspot area data are discussed . in section 5 the problem of the existence of the periodicity of about 155 days during the maximum activity period for sunspot data from the whole solar disk and from each solar hemisphere separately is considered . to find periodicities in a given time series the power spectrum analysis is applied . in this paper two methods are used : the fast fourier transformation algorithm with the hamming window function ( fft ) and the blackman - tukey ( bt ) power spectrum method @xcite . the bt method is used for the diagnosis of the reasons of the existence of peaks , which are obtained by the fft method . the bt method consists in the smoothing of a cosine transform of an autocorrelation function using a 3-point weighting average . such an estimator is consistent and unbiased . moreover , the peaks are uncorrelated and their sum is a variance of a considered time series . the main disadvantage of this method is a weak resolution of the periodogram points , particularly for low frequences . for example , if the autocorrelation function is evaluated for @xmath8 , then the distribution points in the time domain are : @xmath9 thus , it is obvious that this method should not be used for detecting low frequency periodicities with a fairly good resolution . however , because of an application of the autocorrelation function , the bt method can be used to verify a reality of peaks which are computed using a method giving the better resolution ( for example the fft method ) . it is valuable to remember that the power spectrum methods should be applied very carefully . the difficulties in the interpretation of significant peaks could be caused by at least four effects : a sampling of a continuos function , an echo - effect , a contribution of long - term periodicities and a random noise . first effect exists because periodicities , which are shorter than the sampling interval , may mix with longer periodicities . in result , this effect can be reduced by an decrease of the sampling interval between observations . the echo - effect occurs when there is a latent harmonic of frequency @xmath10 in the time series , giving a spectral peak at @xmath10 , and also periodic terms of frequency @xmath11 etc . this may be detected by the autocorrelation function for time series with a large variance . time series often contain long - term periodicities , that influence short - term peaks . they could rise periodogram s peaks at lower frequencies . however , it is also easy to notice the influence of the long - term periodicities on short - term peaks in the graphs of the autocorrelation functions . this effect is observed for the time series of solar activity indexes which are limited by the 11-year cycle . to find statistically significant periodicities it is reasonable to use the autocorrelation function and the power spectrum method with a high resolution . in the case of a stationary time series they give similar results . moreover , for a stationary time series with the mean zero the fourier transform is equivalent to the cosine transform of an autocorrelation function @xcite . thus , after a comparison of a periodogram with an appropriate autocorrelation function one can detect peaks which are in the graph of the first function and do not exist in the graph of the second function . the reasons of their existence could be explained by the long - term periodicities and the echo - effect . below method enables one to detect these effects . ( solid line ) and the 95% confidence level basing on thered noise ( dotted line ) . the periodogram values are presented on the left axis . the lower curve illustrates the autocorrelation function of the same time series ( solid line ) . the dotted lines represent two standard errors of the autocorrelation function . the dashed horizontal line shows the zero level . the autocorrelation values are shown in the right axis . ] because the statistical tests indicate that the time series is a white noise the confidence level is not marked . ] . ] the method of the diagnosis of an echo - effect in the power spectrum ( de ) consists in an analysis of a periodogram of a given time series computed using the bt method . the bt method bases on the cosine transform of the autocorrelation function which creates peaks which are in the periodogram , but not in the autocorrelation function . the de method is used for peaks which are computed by the fft method ( with high resolution ) and are statistically significant . the time series of sunspot activity indexes with the spacing interval one rotation or one month contain a markov - type persistence , which means a tendency for the successive values of the time series to remember their antecendent values . thus , i use a confidence level basing on the red noise of markov @xcite for the choice of the significant peaks of the periodogram computed by the fft method . when a time series does not contain the markov - type persistence i apply the fisher test and the kolmogorov - smirnov test at the significance level @xmath12 @xcite to verify a statistically significance of periodograms peaks . the fisher test checks the null hypothesis that the time series is white noise agains the alternative hypothesis that the time series contains an added deterministic periodic component of unspecified frequency . because the fisher test tends to be severe in rejecting peaks as insignificant the kolmogorov - smirnov test is also used . the de method analyses raw estimators of the power spectrum . they are given as follows @xmath13 for @xmath14 + where @xmath15 for @xmath16 + @xmath17 is the length of the time series @xmath18 and @xmath19 is the mean value . the first term of the estimator @xmath20 is constant . the second term takes two values ( depending on odd or even @xmath21 ) which are not significant because @xmath22 for large m. thus , the third term of ( 1 ) should be analysed . looking for intervals of @xmath23 for which @xmath24 has the same sign and different signs one can find such parts of the function @xmath25 which create the value @xmath20 . let the set of values of the independent variable of the autocorrelation function be called @xmath26 and it can be divided into the sums of disjoint sets : @xmath27 where + @xmath28 + @xmath29 @xmath30 @xmath31 + @xmath32 + @xmath33 @xmath34 @xmath35 @xmath36 @xmath37 @xmath38 @xmath39 @xmath40 well , the set @xmath41 contains all integer values of @xmath23 from the interval of @xmath42 for which the autocorrelation function and the cosinus function with the period @xmath43 $ ] are positive . the index @xmath44 indicates successive parts of the cosinus function for which the cosinuses of successive values of @xmath23 have the same sign . however , sometimes the set @xmath41 can be empty . for example , for @xmath45 and @xmath46 the set @xmath47 should contain all @xmath48 $ ] for which @xmath49 and @xmath50 , but for such values of @xmath23 the values of @xmath51 are negative . thus , the set @xmath47 is empty . . the periodogram values are presented on the left axis . the lower curve illustrates the autocorrelation function of the same time series . the autocorrelation values are shown in the right axis . ] let us take into consideration all sets \{@xmath52 } , \{@xmath53 } and \{@xmath41 } which are not empty . because numberings and power of these sets depend on the form of the autocorrelation function of the given time series , it is impossible to establish them arbitrary . thus , the sets of appropriate indexes of the sets \{@xmath52 } , \{@xmath53 } and \{@xmath41 } are called @xmath54 , @xmath55 and @xmath56 respectively . for example the set @xmath56 contains all @xmath44 from the set @xmath57 for which the sets @xmath41 are not empty . to separate quantitatively in the estimator @xmath20 the positive contributions which are originated by the cases described by the formula ( 5 ) from the cases which are described by the formula ( 3 ) the following indexes are introduced : @xmath58 @xmath59 @xmath60 @xmath61 where @xmath62 @xmath63 @xmath64 taking for the empty sets \{@xmath53 } and \{@xmath41 } the indices @xmath65 and @xmath66 equal zero . the index @xmath65 describes a percentage of the contribution of the case when @xmath25 and @xmath51 are positive to the positive part of the third term of the sum ( 1 ) . the index @xmath66 describes a similar contribution , but for the case when the both @xmath25 and @xmath51 are simultaneously negative . thanks to these one can decide which the positive or the negative values of the autocorrelation function have a larger contribution to the positive values of the estimator @xmath20 . when the difference @xmath67 is positive , the statement the @xmath21-th peak really exists can not be rejected . thus , the following formula should be satisfied : @xmath68 because the @xmath21-th peak could exist as a result of the echo - effect , it is necessary to verify the second condition : @xmath69\in c_m.\ ] ] . the periodogram values are presented on the left axis . the lower curve illustrates the autocorrelation function of the same time series ( solid line ) . the dotted lines represent two standard errors of the autocorrelation function . the dashed horizontal line shows the zero level . the autocorrelation values are shown in the right axis . ] to verify the implication ( 8) firstly it is necessary to evaluate the sets @xmath41 for @xmath70 of the values of @xmath23 for which the autocorrelation function and the cosine function with the period @xmath71 $ ] are positive and the sets @xmath72 of values of @xmath23 for which the autocorrelation function and the cosine function with the period @xmath43 $ ] are negative . secondly , a percentage of the contribution of the sum of products of positive values of @xmath25 and @xmath51 to the sum of positive products of the values of @xmath25 and @xmath51 should be evaluated . as a result the indexes @xmath65 for each set @xmath41 where @xmath44 is the index from the set @xmath56 are obtained . thirdly , from all sets @xmath41 such that @xmath70 the set @xmath73 for which the index @xmath65 is the greatest should be chosen . the implication ( 8) is true when the set @xmath73 includes the considered period @xmath43 $ ] . this means that the greatest contribution of positive values of the autocorrelation function and positive cosines with the period @xmath43 $ ] to the periodogram value @xmath20 is caused by the sum of positive products of @xmath74 for each @xmath75-\frac{m}{2k},[\frac{2m}{k}]+\frac{m}{2k})$ ] . when the implication ( 8) is false , the peak @xmath20 is mainly created by the sum of positive products of @xmath74 for each @xmath76-\frac{m}{2k},\big [ \frac{2m}{n}\big ] + \frac{m}{2k } \big ) $ ] , where @xmath77 is a multiple or a divisor of @xmath21 . it is necessary to add , that the de method should be applied to the periodograms peaks , which probably exist because of the echo - effect . it enables one to find such parts of the autocorrelation function , which have the significant contribution to the considered peak . the fact , that the conditions ( 7 ) and ( 8) are satisfied , can unambiguously decide about the existence of the considered periodicity in the given time series , but if at least one of them is not satisfied , one can doubt about the existence of the considered periodicity . thus , in such cases the sentence the peak can not be treated as true should be used . using the de method it is necessary to remember about the power of the set @xmath78 . if @xmath79 is too large , errors of an autocorrelation function estimation appear . they are caused by the finite length of the given time series and as a result additional peaks of the periodogram occur . if @xmath79 is too small , there are less peaks because of a low resolution of the periodogram . in applications @xmath80 is used . in order to evaluate the value @xmath79 the fft method is used . the periodograms computed by the bt and the fft method are compared . the conformity of them enables one to obtain the value @xmath79 . . the fft periodogram values are presented on the left axis . the lower curve illustrates the bt periodogram of the same time series ( solid line and large black circles ) . the bt periodogram values are shown in the right axis . ] in this paper the sunspot activity data ( august 1923 - october 1933 ) provided by the greenwich photoheliographic results ( gpr ) are analysed . firstly , i consider the monthly sunspot number data . to eliminate the 11-year trend from these data , the consecutively smoothed monthly sunspot number @xmath81 is subtracted from the monthly sunspot number @xmath82 where the consecutive mean @xmath83 is given by @xmath84 the values @xmath83 for @xmath85 and @xmath86 are calculated using additional data from last six months of cycle 15 and first six months of cycle 17 . because of the north - south asymmetry of various solar indices @xcite , the sunspot activity is considered for each solar hemisphere separately . analogously to the monthly sunspot numbers , the time series of sunspot areas in the northern and southern hemispheres with the spacing interval @xmath87 rotation are denoted . in order to find periodicities , the following time series are used : + @xmath88 + @xmath89 + @xmath90 + in the lower part of figure [ f1 ] the autocorrelation function of the time series for the northern hemisphere @xmath88 is shown . it is easy to notice that the prominent peak falls at 17 rotations interval ( 459 days ) and @xmath25 for @xmath91 $ ] rotations ( [ 81 , 162 ] days ) are significantly negative . the periodogram of the time series @xmath88 ( see the upper curve in figures [ f1 ] ) does not show the significant peaks at @xmath92 rotations ( 135 , 162 days ) , but there is the significant peak at @xmath93 ( 243 days ) . the peaks at @xmath94 are close to the peaks of the autocorrelation function . thus , the result obtained for the periodicity at about @xmath0 days are contradict to the results obtained for the time series of daily sunspot areas @xcite . for the southern hemisphere ( the lower curve in figure [ f2 ] ) @xmath25 for @xmath95 $ ] rotations ( [ 54 , 189 ] days ) is not positive except @xmath96 ( 135 days ) for which @xmath97 is not statistically significant . the upper curve in figures [ f2 ] presents the periodogram of the time series @xmath89 . this time series does not contain a markov - type persistence . moreover , the kolmogorov - smirnov test and the fisher test do not reject a null hypothesis that the time series is a white noise only . this means that the time series do not contain an added deterministic periodic component of unspecified frequency . the autocorrelation function of the time series @xmath90 ( the lower curve in figure [ f3 ] ) has only one statistically significant peak for @xmath98 months ( 480 days ) and negative values for @xmath99 $ ] months ( [ 90 , 390 ] days ) . however , the periodogram of this time series ( the upper curve in figure [ f3 ] ) has two significant peaks the first at 15.2 and the second at 5.3 months ( 456 , 159 days ) . thus , the periodogram contains the significant peak , although the autocorrelation function has the negative value at @xmath100 months . to explain these problems two following time series of daily sunspot areas are considered : + @xmath101 + @xmath102 + where @xmath103 the values @xmath104 for @xmath105 and @xmath106 are calculated using additional daily data from the solar cycles 15 and 17 . and the cosine function for @xmath45 ( the period at about 154 days ) . the horizontal line ( dotted line ) shows the zero level . the vertical dotted lines evaluate the intervals where the sets @xmath107 ( for @xmath108 ) are searched . the percentage values show the index @xmath65 for each @xmath41 for the time series @xmath102 ( in parentheses for the time series @xmath101 ) . in the right bottom corner the values of @xmath65 for the time series @xmath102 , for @xmath109 are written . ] ( the 500-day period ) ] the comparison of the functions @xmath25 of the time series @xmath101 ( the lower curve in figure [ f4 ] ) and @xmath102 ( the lower curve in figure [ f5 ] ) suggests that the positive values of the function @xmath110 of the time series @xmath101 in the interval of @xmath111 $ ] days could be caused by the 11-year cycle . this effect is not visible in the case of periodograms of the both time series computed using the fft method ( see the upper curves in figures [ f4 ] and [ f5 ] ) or the bt method ( see the lower curve in figure [ f6 ] ) . moreover , the periodogram of the time series @xmath102 has the significant values at @xmath112 days , but the autocorrelation function is negative at these points . @xcite showed that the lomb - scargle periodograms for the both time series ( see @xcite , figures 7 a - c ) have a peak at 158.8 days which stands over the fap level by a significant amount . using the de method the above discrepancies are obvious . to establish the @xmath79 value the periodograms computed by the fft and the bt methods are shown in figure [ f6 ] ( the upper and the lower curve respectively ) . for @xmath46 and for periods less than 166 days there is a good comformity of the both periodograms ( but for periods greater than 166 days the points of the bt periodogram are not linked because the bt periodogram has much worse resolution than the fft periodogram ( no one know how to do it ) ) . for @xmath46 and @xmath113 the value of @xmath21 is 13 ( @xmath71=153 $ ] ) . the inequality ( 7 ) is satisfied because @xmath114 . this means that the value of @xmath115 is mainly created by positive values of the autocorrelation function . the implication ( 8) needs an evaluation of the greatest value of the index @xmath65 where @xmath70 , but the solar data contain the most prominent period for @xmath116 days because of the solar rotation . thus , although @xmath117 for each @xmath118 , all sets @xmath41 ( see ( 5 ) and ( 6 ) ) without the set @xmath119 ( see ( 4 ) ) , which contains @xmath120 $ ] , are considered . this situation is presented in figure [ f7 ] . in this figure two curves @xmath121 and @xmath122 are plotted . the vertical dotted lines evaluate the intervals where the sets @xmath107 ( for @xmath123 ) are searched . for such @xmath41 two numbers are written : in parentheses the value of @xmath65 for the time series @xmath101 and above it the value of @xmath65 for the time series @xmath102 . to make this figure clear the curves are plotted for the set @xmath124 only . ( in the right bottom corner information about the values of @xmath65 for the time series @xmath102 , for @xmath109 are written . ) the implication ( 8) is not true , because @xmath125 for @xmath126 . therefore , @xmath43=153\notin c_6=[423,500]$ ] . moreover , the autocorrelation function for @xmath127 $ ] is negative and the set @xmath128 is empty . thus , @xmath129 . on the basis of these information one can state , that the periodogram peak at @xmath130 days of the time series @xmath102 exists because of positive @xmath25 , but for @xmath23 from the intervals which do not contain this period . looking at the values of @xmath65 of the time series @xmath101 , one can notice that they decrease when @xmath23 increases until @xmath131 . this indicates , that when @xmath23 increases , the contribution of the 11-year cycle to the peaks of the periodogram decreases . an increase of the value of @xmath65 is for @xmath132 for the both time series , although the contribution of the 11-year cycle for the time series @xmath101 is insignificant . thus , this part of the autocorrelation function ( @xmath133 for the time series @xmath102 ) influences the @xmath21-th peak of the periodogram . this suggests that the periodicity at about 155 days is a harmonic of the periodicity from the interval of @xmath1 $ ] days . ( solid line ) and consecutively smoothed sunspot areas of the one rotation time interval @xmath134 ( dotted line ) . both indexes are presented on the left axis . the lower curve illustrates fluctuations of the sunspot areas @xmath135 . the dotted and dashed horizontal lines represent levels zero and @xmath136 respectively . the fluctuations are shown on the right axis . ] the described reasoning can be carried out for other values of the periodogram . for example , the condition ( 8) is not satisfied for @xmath137 ( 250 , 222 , 200 days ) . moreover , the autocorrelation function at these points is negative . these suggest that there are not a true periodicity in the interval of [ 200 , 250 ] days . it is difficult to decide about the existence of the periodicities for @xmath138 ( 333 days ) and @xmath139 ( 286 days ) on the basis of above analysis . the implication ( 8) is not satisfied for @xmath139 and the condition ( 7 ) is not satisfied for @xmath138 , although the function @xmath25 of the time series @xmath102 is significantly positive for @xmath140 . the conditions ( 7 ) and ( 8) are satisfied for @xmath141 ( figure [ f8 ] ) and @xmath142 . therefore , it is possible to exist the periodicity from the interval of @xmath1 $ ] days . similar results were also obtained by @xcite for daily sunspot numbers and daily sunspot areas . she considered the means of three periodograms of these indexes for data from @xmath143 years and found statistically significant peaks from the interval of @xmath1 $ ] ( see @xcite , figure 2 ) . @xcite studied sunspot areas from 1876 - 1999 and sunspot numbers from 1749 - 2001 with the help of the wavelet transform . they pointed out that the 154 - 158-day period could be the third harmonic of the 1.3-year ( 475-day ) period . moreover , the both periods fluctuate considerably with time , being stronger during stronger sunspot cycles . therefore , the wavelet analysis suggests a common origin of the both periodicities . this conclusion confirms the de method result which indicates that the periodogram peak at @xmath144 days is an alias of the periodicity from the interval of @xmath1 $ ] in order to verify the existence of the periodicity at about 155 days i consider the following time series : + @xmath145 + @xmath146 + @xmath147 + the value @xmath134 is calculated analogously to @xmath83 ( see sect . the values @xmath148 and @xmath149 are evaluated from the formula ( 9 ) . in the upper part of figure [ f9 ] the time series of sunspot areas @xmath150 of the one rotation time interval from the whole solar disk and the time series of consecutively smoothed sunspot areas @xmath151 are showed . in the lower part of figure [ f9 ] the time series of sunspot area fluctuations @xmath145 is presented . on the basis of these data the maximum activity period of cycle 16 is evaluated . it is an interval between two strongest fluctuations e.a . @xmath152 $ ] rotations . the length of the time interval @xmath153 is 54 rotations . if the about @xmath0-day ( 6 solar rotations ) periodicity existed in this time interval and it was characteristic for strong fluctuations from this time interval , 10 local maxima in the set of @xmath154 would be seen . then it should be necessary to find such a value of p for which @xmath155 for @xmath156 and the number of the local maxima of these values is 10 . as it can be seen in the lower part of figure [ f9 ] this is for the case of @xmath157 ( in this figure the dashed horizontal line is the level of @xmath158 ) . figure [ f10 ] presents nine time distances among the successive fluctuation local maxima and the horizontal line represents the 6-rotation periodicity . it is immediately apparent that the dispersion of these points is 10 and it is difficult to find even few points which oscillate around the value of 6 . such an analysis was carried out for smaller and larger @xmath136 and the results were similar . therefore , the fact , that the about @xmath0-day periodicity exists in the time series of sunspot area fluctuations during the maximum activity period is questionable . . the horizontal line represents the 6-rotation ( 162-day ) period . ] ] ] to verify again the existence of the about @xmath0-day periodicity during the maximum activity period in each solar hemisphere separately , the time series @xmath88 and @xmath89 were also cut down to the maximum activity period ( january 1925december 1930 ) . the comparison of the autocorrelation functions of these time series with the appriopriate autocorrelation functions of the time series @xmath88 and @xmath89 , which are computed for the whole 11-year cycle ( the lower curves of figures [ f1 ] and [ f2 ] ) , indicates that there are not significant differences between them especially for @xmath23=5 and 6 rotations ( 135 and 162 days ) ) . this conclusion is confirmed by the analysis of the time series @xmath146 for the maximum activity period . the autocorrelation function ( the lower curve of figure [ f11 ] ) is negative for the interval of [ 57 , 173 ] days , but the resolution of the periodogram is too low to find the significant peak at @xmath159 days . the autocorrelation function gives the same result as for daily sunspot area fluctuations from the whole solar disk ( @xmath160 ) ( see also the lower curve of figures [ f5 ] ) . in the case of the time series @xmath89 @xmath161 is zero for the fluctuations from the whole solar cycle and it is almost zero ( @xmath162 ) for the fluctuations from the maximum activity period . the value @xmath163 is negative . similarly to the case of the northern hemisphere the autocorrelation function and the periodogram of southern hemisphere daily sunspot area fluctuations from the maximum activity period @xmath147 are computed ( see figure [ f12 ] ) . the autocorrelation function has the statistically significant positive peak in the interval of [ 155 , 165 ] days , but the periodogram has too low resolution to decide about the possible periodicities . the correlative analysis indicates that there are positive fluctuations with time distances about @xmath0 days in the maximum activity period . the results of the analyses of the time series of sunspot area fluctuations from the maximum activity period are contradict with the conclusions of @xcite . she uses the power spectrum analysis only . the periodogram of daily sunspot fluctuations contains peaks , which could be harmonics or subharmonics of the true periodicities . they could be treated as real periodicities . this effect is not visible for sunspot data of the one rotation time interval , but averaging could lose true periodicities . this is observed for data from the southern hemisphere . there is the about @xmath0-day peak in the autocorrelation function of daily fluctuations , but the correlation for data of the one rotation interval is almost zero or negative at the points @xmath164 and 6 rotations . thus , it is reasonable to research both time series together using the correlative and the power spectrum analyses . the following results are obtained : 1 . a new method of the detection of statistically significant peaks of the periodograms enables one to identify aliases in the periodogram . 2 . two effects cause the existence of the peak of the periodogram of the time series of sunspot area fluctuations at about @xmath0 days : the first is caused by the 27-day periodicity , which probably creates the 162-day periodicity ( it is a subharmonic frequency of the 27-day periodicity ) and the second is caused by statistically significant positive values of the autocorrelation function from the intervals of @xmath165 $ ] and @xmath166 $ ] days . the existence of the periodicity of about @xmath0 days of the time series of sunspot area fluctuations and sunspot area fluctuations from the northern hemisphere during the maximum activity period is questionable . the autocorrelation analysis of the time series of sunspot area fluctuations from the southern hemisphere indicates that the periodicity of about 155 days exists during the maximum activity period . i appreciate valuable comments from professor j. jakimiec .","the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . a new method of the diagnosis of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days . the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . it proves against the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only ."
"it is believed that the direct detection of gravitational waves ( gws ) will bring the era of gravitational wave astronomy . the interferometer detectors are now under operation and awaiting the first signal of gws @xcite . it is also known that pulsar timing arrays ( ptas ) can be used as a detector for gws @xcite . these detectors are used to search for very low frequency ( @xmath0 ) gravitational waves , where the lower limit of the observable frequencies is determined by the inverse of total observation time @xmath1 . indeed , the total observation time has a crucial role in ptas , because ptas are most sensitive near the lower edge of observable frequencies @xcite . taking into account its sensitivity , the first direct detection of the gravitational waves might be achieved by ptas . the main target of ptas is the stochastic gravitational wave background ( sgwb ) generated by a large number of unresolved sources with the astrophysical origin or the cosmological origin in the early universe . the promising sources are super massive black hole binaries @xcite , cosmic ( super)string @xcite , and inflation @xcite . previous studies have assumed that the sgwb is isotropic and unpolarized @xcite . these assumptions are reasonable for the primary detection of the sgwb , but the deviation from the isotropy and the polarizations should have rich information of sources of gravitational waves . recently , the cross - correlation formalism has been generalized to deal with anisotropy in the sgwb @xcite . result of this work enables us to consider arbitrary levels of anisotropy , and a bayesian approach was performed by using this formalism @xcite . on the other hand , for the anisotropy of the sgwb , the cross - correlation formalism has been also developed in the case of interferometer detectors @xcite . as to the polarization , there are works including the ones motivated by the modified gravity @xcite . we can envisage supermassive black hole binaries emit circularly polarized sgwb due to the chern - simons term @xcite . there may also exist cosmological sgwb with circular polarization in the presence of parity violating term in gravity sector @xcite . in this paper , we investigate the detectability of circular polarization in the sgwb by ptas . we characterize sgwb by the so called stokes @xmath2 parameter @xcite and calculate generalized overlap reduction functions ( orfs ) so that we can probe the circular polarization of the sgwb . we also discuss a method to separate the intensity ( @xmath3 mode ) and circular polarization ( @xmath2 mode ) of the sgwb . the paper is organized as follows . in section [ sec : stokes parameters for a plane gravitational wave ] , we introduce the stokes parameters for monochromatic plane gravitational waves , and clarify the physical meaning of the stokes parameters @xmath3 and @xmath2 . in section [ sec : formulation ] , we formulate the cross - correlation formalism for anisotropic circularly polarized sgwb with ptas . the basic framework is essentially a combination of the formalism of @xcite , and the polarization decomposition formula of the sgwb derived in @xcite . in section [ sec : the generalized overlap reduction function for circular polarization ] , we calculate the generalized orfs for the @xmath2 mode . the results for @xmath3 mode are consistent with the previous work @xcite . in section [ sec : separation method ] , we give a method for separation between the @xmath3 mode and @xmath2 mode of the sgwb . the final section is devoted to the conclusion . in appendixes , we present analytic results for the generalized overlap reduction functions . in this paper , we will use the gravitational units @xmath4 . let us consider the stokes parameters for plane waves traveling in the direction @xmath5 , which can be described by @xmath6 \ , \\ & & h_{xy}(t , z)=h_{yx}(t , z)={\rm re}[b_{\times}\mathrm{e}^{-iw(t - z ) } ] \ .\end{aligned}\ ] ] for an idealized monochromatic plane wave , complex amplitudes @xmath7 and @xmath8 are constants . polarization of the plane gws is characterized by the tensor , ( see @xcite and also electromagnetic case @xcite ) @xmath9 where @xmath10 take @xmath11 . any @xmath12 hermitian matrix can be expanded by the pauli and the unit matrices with real coefficients . hence , the @xmath13 hermitian matrix @xmath14 can be written as @xmath15 where @xmath16 by analogy with electromagnetic cases , @xmath17 and @xmath2 are called stokes parameters . comparing with , we can read off the stokes parameters as @xmath18= b_{+}^{\ast}b_{\times}+ b_{\times}^{\ast}b_{+},\\ v&=&-2{\rm i m } [ b_{+}^{\ast}b_{\times}]=i ( b_{+}^{\ast}b_{\times}- b_{\times}^{\ast}b_{+}).\label{stv}\end{aligned}\ ] ] apparently , the real parameter @xmath3 is the intensity of gws . in order to reveal the physical meaning of the real parameter @xmath2 , we define the circular polarization bases @xcite @xmath19 from the relation @xmath20 we see @xmath21 thus , we can rewrite the stokes parameters - as @xmath22 from the above expression , we see that the real parameter @xmath2 characterizes the asymmetry of circular polarization amplitudes . the other parameters @xmath23 and @xmath24 have additional information about linear polarizations by analogy with the electromagnetic cases . alternatively , we can also define the tensor @xmath25 in circular polarization bases @xmath26 where @xmath27 . note that the stokes parameters satisfy a relation @xmath28 next , we consider the transformation of the stokes parameters under rotations around the @xmath5 axis . the rotation around the @xmath5 axis is given by @xmath29 where @xmath30 is the angle of the rotation . the gws traveling in the direction @xmath5 @xmath31 transform as @xmath32 where we took the transverse traceless gauge @xmath33 after a short calculation , we obtain @xmath34 using and , the four stokes parameters ( [ sti])-([stv ] ) transform as @xmath35 as you can see , the parameters @xmath23 and @xmath24 depend on the rotation angle @xmath30 . this reflects the fact that @xmath23 and @xmath24 parameters characterize linear polarizations . note that this transformation is similar to the transformation of electromagnetic case except for the angle @xmath36 and can be rewritten as @xmath37 in this section , we study anisotropic distribution of sgwb and focus on the detectability of circular polarizations with pulsar timing arrays . we combine the analysis of @xcite and that of @xcite . in sec.[subsec : the spectral ] , we derive the power spectral density for anisotropic circularly polarized sgwb @xmath38 . then we also derive the dimensionless density parameter @xmath39 which is expressed by the frequency spectrum of intensity @xmath40 @xcite . in sec.[subsec : the signal ] , we extend the generalized orfs to cases with circular polarizations characterized by the parameter @xmath2 . for simplicity , we consider specific anisotropic patterns with @xmath41 expressed by the spherical harmonics @xmath42 . in the transverse traceless gauge , metric perturbations @xmath43 with a given propagation direction @xmath44 can be expanded as @xcite @xmath45 where the fourier amplitude satisfies @xmath46 as a consequence of the reality of @xmath43 , @xmath47 , @xmath48 is the frequency of the gws , @xmath49 are spatial indices , @xmath50 label polarizations . note that the fourier amplitude @xmath51 satisfies the relation @xmath52 where @xmath53 was defined by . the polarized tensors @xmath54 are defined by @xmath55 where @xmath56 and @xmath57 are unit orthogonal vectors perpendicular to @xmath58 . the polarization tensors satisfy @xmath59 with polar coordinates , the direction @xmath44 can be represented by @xmath60 and the polarization basis vectors read @xmath61 we assume the fourier amplitudes @xmath62 are random variables , which is stationary and gaussian . however , they are not isotropic and unpolarized . the ensemble average of fourier amplitudes can be written as @xcite @xmath63 where @xmath64 here , the bracket @xmath65 represents an ensemble average , and @xmath66 is the dirac delta function on the two - sphere . the gw power spectral density @xmath38 is a hermitian matrix , and satisfies @xmath67 because of the relation @xmath46 . therefore , we have the relations @xmath68 note that the stokes parameters are not exactly the same as the expression of , but they have the relation and characterize the same polarization . we further assume that the sgwbs satisfy @xmath69 we also assume the directional dependence of the sgwb is frequency independent @xcite . this implies the gw power spectral density is factorized into two parts , one of which depends on the direction while the other depends on the frequency . because of the transformations - , the parameters @xmath3 and @xmath2 have spin 0 and the parameters @xmath70 have spin @xmath71 @xcite . to analyze the sgwb on the sky , it is convenient to expand the stokes parameters by spherical harmonics @xmath72 . however , since @xmath70 parameters have spin @xmath71 , they have to be expanded by the spin - weighted harmonics @xmath73 @xcite . thus , we obtain @xmath74 in this paper , we study specific anisotropic patterns with @xmath41 for simplicity . therefore , we can neglect @xmath23 and @xmath24 from now on . thus , the gw power spectral density becomes @xmath75 where @xmath76 so , we focus on the parameters @xmath3 and @xmath2 . in what follows , we will use the following shorthand notation @xmath77 next , we consider the dimensionless density parameter @xcite @xmath78 where @xmath79 is the critical density , @xmath80 is the present value of the hubble parameter , @xmath81 is the energy density of gravitational waves , and @xmath82 is the energy density in the frequency range @xmath48 to @xmath83 . the bracket @xmath65 represents the ensemble average . however , actually , we take a spatial average over the wave lengths @xmath84 of gws or a temporal average over the periods @xmath85 of gws . here , we assumed the ergodicity , namely , the ensemble average can be replaced by the temporal average . using , , , as well as @xmath46 and @xmath86 , we get @xmath87 then we define @xmath88 hence , the dimensionless quantity @xmath39 in is given by @xmath89 where the spherical harmonics are orthogonal and normalized as @xmath90 using @xmath91 , we obtain @xmath92 without loss of generality , we normalize the monopole moment as @xmath93 so , becomes @xmath94 the time of arrival of radio pulses from the pulsar is affected by gws . consider a pulsar with frequency @xmath95 located in the direction @xmath96 . to detect the sgwb , let us consider the redshift of the pulse from a pulsar @xcite @xmath97 where @xmath98 is a frequency detected at the earth and @xmath96 is the direction to the pulsar . the unit vector @xmath44 represents the direction of propagation of gravitational plane waves . we also defined the difference between the metric perturbations at the pulsar @xmath99 and at the earth @xmath100 as @xmath101 the gravitational plane waves at each point is defined as @xmath102 for the sgwb , the redshift have to be integrated over the direction of propagation of the gravitational waves @xmath44 : @xmath103 we choose a coordinate system @xmath104 and assume that the amplitudes of the metric perturbation at the pulsar and the earth are the same . then becomes @xmath105 and therefore , reads @xmath106 where we have defined the pattern functions for pulsars @xmath107 note that our convention for the fourier transformation is @xmath108 therefore , the fourier transformation of can be written as @xmath109 in the actual signals from a pulsar , there exist noises . hence , we need to use the correlation analysis . we consider the signals from two pulsars @xmath110 where @xmath111 labels the pulsar . here , @xmath112 denotes the signal from the pulsar and @xmath113 denotes the noise intrinsic to the measurement . we assume the noises are stationary , gaussian and are not correlated between the two pulsars . to correlate the signals of two measurements , we define @xmath114 where @xmath1 is the total observation time and @xmath115 is a real filter function which should be optimal to maximize signal - to - noise ratio . in the case of interferometer , the optimal filter function falls to zero for large @xmath116 compered to the travel time of the light between the detecters . since the signals of two detectors are expected to correlate due to the same effect of the gravitational waves , the optimal filter function should behave this way . then , typically one of the detectors is very close to the other compared to the total observation time @xmath1 . therefore , the total observation time @xmath1 can be extended to @xmath117 @xcite . in contrast , in the case of pta , it is invalid that @xmath1 is very large compered to the travel time of the light between the pulsars . nevertheless , we can assume that one of the two @xmath1 can be expanded to @xmath117 , because in situations @xmath118 and @xmath119 it is known that we can ignore the effect of the distance @xmath120 of pulsars . in this case , it is clear that any locations of the pulsars are optimal and optimal filter function should behave like as the interferometer case @xcite . using these assumptions @xmath118 and @xmath119 , we can rewrite as @xmath121 where @xmath122 note that @xmath123 satisfies @xmath124 , because @xmath125 is real . moreover , to deal with the unphysical region @xmath126 we require @xmath127 . thus , @xmath123 becomes real . taking the ensemble average , using @xmath128 , @xmath118 , and assuming the noises in the two measurements are not correlated , we get @xmath129\ , \label{s2}\end{aligned}\ ] ] where we have defined @xmath130 the functions @xmath131 and @xmath132 are called the generalized orfs , which describe the angular sensitivity of the pulsars for the sgwb . note that , as we already mentioned , we consider the cases of @xmath41 for simplicity . then we have assumed @xmath118 and @xmath119 , this assumption implies that approximately becomes @xmath133 due to the rapid oscillation of the phase factor . therefore , the distance @xmath120 of the pulsars does not appear in the generalized orfs , and hence the generalized orfs do not depend on the frequency . as you can see from , the correlation of the two measurements involve both the total intensity and the circular polarization . however , the degeneracy can be disentangled by using separation method , which will be discussed in the section [ sec : separation method ] . in this section , we consider the generalized orfs for circular polarizations : @xmath134 where we defined @xmath135 in the above , we have used and the fact that the generalized orfs do not depend on frequency . for computation of the generalized orfs for circular polarizations , it is convenient to use the computational frame @xcite defined by @xmath136 where @xmath137 is the angular separation between the two pulsars . using - , , and , one can easily show that @xmath138 we therefore get @xmath139 the explicit form of the spherical harmonics reads @xmath140 where @xmath141 is the normalization factor . the associated legendre functions are given by @xmath142 and @xmath143 with the legendre functions @xmath144\ .\label{pl}\end{aligned}\ ] ] using the spherical harmonics , becomes @xmath145 where we have used the fact that the function of @xmath146 is odd parity in the case of @xmath147 and is even parity in the case of @xmath148 . note that the generalized orfs for circular polarizations are real functions . in the case of @xmath149 and/or @xmath150 , the integrand in vanishes . therefore , we can not detect circular polarizations for these cases . this fact for @xmath151 implies that we do not need to consider auto - correlation for a single pulsar . this is the reason why we neglected auto - correlation term in . integrating , we get the following form for @xmath152 : @xmath153 for @xmath154 , we have obtained @xmath155 \ , \\ \gamma^{v}_{1 - 1}&=&\gamma^{v}_{11 } \ , \end{aligned}\ ] ] recall that @xmath156 . the derivation of this formula for @xmath154 can be found in appendix [ sec : angular integral of the generalized overlap reduction function for dipole circular polarization ] . for @xmath157 , we derived the following : @xmath158\ , \\ \gamma^{v}_{2 - 1}&=&\gamma^{v}_{21}\ , \\ \gamma^{v}_{22}&=&-\frac{\sqrt{30\pi}}{6}(1-\cos\xi)\left[2-\cos\xi+6\left(\frac{1-\cos\xi}{1+\cos\xi}\right)\log\left(\sin\frac{\xi}{2}\right)\right]\ , \\ \gamma^{v}_{2 - 2}&=&-\gamma^{v}_{22}\ , \end{aligned}\ ] ] for @xmath159 , the results are @xmath160\ , \\ \gamma^{v}_{3 - 1}&=&\gamma^{v}_{31}\ , \\ \gamma^{v}_{32}&=&\frac{\sqrt{210\pi}}{24}(1-\cos\xi)\left[8 - 5\cos\xi-\cos^2\xi+24\left(\frac{1-\cos\xi}{1+\cos\xi}\right)\log\left(\sin\frac{\xi}{2}\right)\right]\ , \\ \gamma^{v}_{3 - 2}&=&-\gamma^{v}_{3 - 2}\ , \\ \gamma^{v}_{33}&=&-\frac{\sqrt{35\pi}}{16}\sin\xi\left(\frac{1-\cos\xi}{1+\cos\xi}\right)\left[11 - 6\cos\xi-\cos^2\xi+32\left(\frac{1-\cos\xi}{1+\cos\xi}\right)\log\left(\sin\frac{\xi}{2}\right)\right]\ , \\ \gamma^{v}_{3 - 3}&=&\gamma^{v}_{33}\ .\end{aligned}\ ] ] in fig . [ gv ] , we plotted these generalized orfs as a function of the angular separation between the two pulsars @xmath137 . it is apparent that considering the @xmath2 mode does not make sense when we only consider the isotropic ( @xmath152 ) orf . on the other hand , when we consider anisotropic ( @xmath161 ) orfs , it is worth taking into account polarizations . the polarizations of the sgwb would give us rich information both of super massive black hole binaries and of inflation in the early universe . as a function of the angular separation between the two pulsars @xmath137 . in fig . [ gv](a ) , we find the orf for the monopole ( l=0 ) is trivial . in fig . [ gv](b ) , the orfs for the dipole ( l=1 ) are shown . in fig . [ gv](c ) , the orfs for the quadrupole ( l=2 ) are depicted . in fig . [ gv](d ) , the orfs for the octupole ( l=3 ) are plotted . the black solid curve , the blue dashed curve , the red dotted curve , the dark - red space - dotted curve , and the green long - dashed curve represent @xmath149 , @xmath162 , @xmath163 , @xmath164 , @xmath165 , respectively.,title=""fig:"",width=340 ] ( a ) @xmath152 as a function of the angular separation between the two pulsars @xmath137 . in fig . [ gv](a ) , we find the orf for the monopole ( l=0 ) is trivial . in fig . [ gv](b ) , the orfs for the dipole ( l=1 ) are shown . in fig . [ gv](c ) , the orfs for the quadrupole ( l=2 ) are depicted . in fig . [ gv](d ) , the orfs for the octupole ( l=3 ) are plotted . the black solid curve , the blue dashed curve , the red dotted curve , the dark - red space - dotted curve , and the green long - dashed curve represent @xmath149 , @xmath162 , @xmath163 , @xmath164 , @xmath165 , respectively.,title=""fig:"",width=340 ] ( b ) @xmath154 as a function of the angular separation between the two pulsars @xmath137 . in fig . [ gv](a ) , we find the orf for the monopole ( l=0 ) is trivial . in fig . [ gv](b ) , the orfs for the dipole ( l=1 ) are shown . in fig . [ gv](c ) , the orfs for the quadrupole ( l=2 ) are depicted . in fig . [ gv](d ) , the orfs for the octupole ( l=3 ) are plotted . the black solid curve , the blue dashed curve , the red dotted curve , the dark - red space - dotted curve , and the green long - dashed curve represent @xmath149 , @xmath162 , @xmath163 , @xmath164 , @xmath165 , respectively.,title=""fig:"",width=340 ] ( c ) @xmath157 as a function of the angular separation between the two pulsars @xmath137 . in fig . [ gv](a ) , we find the orf for the monopole ( l=0 ) is trivial . in fig . [ gv](b ) , the orfs for the dipole ( l=1 ) are shown . in fig . [ gv](c ) , the orfs for the quadrupole ( l=2 ) are depicted . in fig . [ gv](d ) , the orfs for the octupole ( l=3 ) are plotted . the black solid curve , the blue dashed curve , the red dotted curve , the dark - red space - dotted curve , and the green long - dashed curve represent @xmath149 , @xmath162 , @xmath163 , @xmath164 , @xmath165 , respectively.,title=""fig:"",width=340 ] ( d ) @xmath159 using the same procedure described in the above to derive the generalized orfs for circular polarizations , we can also derive the generalized orfs for the intensity @xmath166 where @xmath167 the angular integral in this case was performed in @xcite . the results are summarized in appendix [ sec : the generalized overlap reduction function for intensity ] . in this section , we separate the @xmath3 mode and @xmath2 mode of the sgwb with correlation analysis @xcite . to this aim , we use four pulsars ( actually we need at least three pulsars ) , and define correlations of @xmath168 @xmath169 where @xmath170 label the pulsars . comparing with , we obtain @xmath171 \ , \label{1c12}\\ & & c_{34}(f)=\sum_{lm}^{l=3}\left[c_{lm}^{i}i(f)\gamma_{lm,34}^{i}+c_{lm}^{v}v(f)\gamma_{lm,34}^{v}\right ] \ .\label{1c34}\end{aligned}\ ] ] if the @xmath3 mode and @xmath2 mode of the sgwb are dominated by a certain @xmath172 and @xmath173 , and become @xmath174 \ , \label{2c12 } \\ & & c_{34}(f)=\left[c _ { l m}^{i}i(f)\gamma _ { l m,34}^{i}+c _ { l ' m'}^{v}v(f)\gamma _ { l ' m',34}^{v}\right ] \ .\label{2c34}\end{aligned}\ ] ] to separate the intensity and the circular polarization , we take the following linear combinations @xmath175 where we defined coefficients @xmath176 as you can see , @xmath177 contains only @xmath40 , and @xmath178 contains only @xmath179 . for the signal @xmath180 , the formulas corresponding to and are given by @xmath181 \ , \label{sp}\end{aligned}\ ] ] where @xmath182 denotes @xmath3 and @xmath2 . we assume @xmath183 and that the noise in the four pulsars are not correlated . we also assume that the ensemble average of fourier amplitudes of the noises @xmath184 is of the form @xmath185 where @xmath186 is the noise power spectral density . the reality of @xmath187 gives rise to @xmath188 and therefore we obtain @xmath189 . without loss of generality , we can assume @xmath190 then we obtain corresponding noises @xmath191 : @xmath192\ , \label{np}\end{aligned}\ ] ] where @xmath193^{1/2 } \label{sn12 } \ , \quad s_{n,34}(f ) \equiv [ s_{n,3}(f)s_{n,4}(f)]^{1/2 } \label{sn34 } \ .\end{aligned}\ ] ] using the inner product @xmath194 \ , \end{aligned}\ ] ] we can rewrite , as @xmath195 therefore , the optimal filter function can be chosen as @xmath196 using , we get optimal signal - to - noise ratio @xmath197^{1/2}\ .\label{snr}\end{aligned}\ ] ] plugging , , and into , we obtain @xmath198^{1/2}\ , \\ { \rm snr}_{v}&=&\left[t\int_{-\infty}^{\infty}df\,\,\frac{\left(c^{v}_{{l}'{m}'}\right)^{2}v^{2}(f)\left(\gamma_{{l}'{m}',34}^{v}\gamma^{i}_{{l}{m},12}-\gamma_{{l}'{m}',12}^{v}\gamma^{i}_{{l}{m},34}\right)^2}{\left(\gamma^{i}_{{l}{m},12}\right)^2s^{2}_{n,34}(f)+\left(\gamma^{i}_{{l}{m},34}\right)^2s^{2}_{n,12}(f)}\right]^{1/2}\ .\end{aligned}\ ] ] if we assume all of the noise power spectral densities are the same , becomes @xmath199 thus , the compiled orfs can be defined as @xmath200^{1/2}}\ , \\ \gamma_{12:34}^{v}&\equiv&\frac{\gamma_{{l}'{m}',34}^{v}\gamma^{i}_{{l}{m},12}-\gamma_{{l}'{m}',12}^{v}\gamma^{i}_{{l}{m},34}}{\left[\left(\gamma^{i}_{{l}{m},12}\right)^2+\left(\gamma^{i}_{{l}{m},34}\right)^2\right]^{1/2}}\ .\end{aligned}\ ] ] this compiled orfs @xmath201 and @xmath202 describe the angular sensitivity of the four pulsars for the pure @xmath3 and @xmath2 mode of the sgwb , respectively . note that , to do this separation , we must know a priori the coefficients @xmath203 and @xmath204 . if we do not assume , the generalized orfs depend on the frequency . in this case , it seems difficult to calculate these coefficients . we next consider the case that @xmath3 mode and/or @xmath2 mode dominant in two or more @xmath205 . in this case , if we have a priori knowledge of the values of @xmath206 in each of @xmath205 for coefficients @xmath203 and @xmath204 , we can separate @xmath3 mode and @xmath2 mode . for example , assume that @xmath3 mode is dominated by @xmath207 , while @xmath2 mode is dominated by @xmath208 , then and become @xmath209\ , \label{3c12}\\ & & c_{34}(f)=\left[c^{i}_{00}i(f)\left(\gamma_{00,34}^{i}+\frac{c^{i}_{11}}{c^{i}_{00}}\gamma_{11,34}^{i}\right)+c_{11}^{v}v(f)\gamma_{11,34}^{v}\right]\ .\label{3c34}\end{aligned}\ ] ] thus , we can separate @xmath3 mode and @xmath2 mode by using linear combinations @xmath210\ , \\ d_{v}&\equiv&a_{v}c_{34}(f)+b_{v}c_{12}(f ) \nonumber\\ & = & c_{11}^{v}v(f)\left[\gamma_{11,34}^{v}\left(\gamma_{00,12}^{i}+\frac{c^{i}_{11}}{c^{i}_{00}}\gamma_{11,12}^{i}\right)-\gamma_{11,12}^{v}\left(\gamma_{00,34}^{i}+\frac{c^{i}_{11}}{c^{i}_{00}}\gamma_{11,34}^{i}\right)\right]\ , \end{aligned}\ ] ] where @xmath211 as in the previous calculations , we can get the compiled orfs @xmath212^{1/2}}\ , \label{gi1234}\\ \gamma_{12:34}^{v}&\equiv&\frac{\gamma_{11,34}^{v}\left(\gamma_{00,12}^{i}+\displaystyle\frac{c^{i}_{11}}{c^{i}_{00}}\gamma_{11,12}^{i}\right)-\gamma_{11,12}^{v}\left(\gamma_{00,34}^{i}+\displaystyle\frac{c^{i}_{11}}{c^{i}_{00}}\gamma_{11,34}^{i}\right)}{\left[\left(\gamma_{00,12}^{i}+\displaystyle\frac{c^{i}_{11}}{c^{i}_{00}}\gamma_{11,12}^{i}\right)^2+\left(\gamma_{00,34}^{i}+\displaystyle\frac{c^{i}_{11}}{c^{i}_{00}}\gamma_{11,34}^{i}\right)^2\right]^{1/2}}\ .\label{gv1234}\end{aligned}\ ] ] [ cols=""^,^ "" , ] in fig . [ cg ] we show some compiled orfs @xmath213 ( left panels ) and @xmath214 ( right panels ) as a function of the two angular separations @xmath137 and @xmath215 for two pulsar pairs , respectively . we used the expressions of @xmath2 mode and @xmath3 mode ( see appendix [ sec : the generalized overlap reduction function for intensity ] ) , and we assumed @xmath216 for simplicity . in fig . [ cg](a ) and [ cg](b ) , the @xmath3 mode is dominated by @xmath217 and @xmath2 mode is dominated by @xmath218 . in fig . [ cg](c ) and [ cg](d ) , the @xmath3 mode is dominated by @xmath219 and @xmath2 mode is dominated by @xmath218 . in fig . [ cg](e ) and [ cg](f ) , the @xmath3 mode is dominated by @xmath207 and @xmath2 mode is dominated by @xmath218 . in fig . [ cg](e ) and [ cg](f ) , the @xmath3 mode is dominated by @xmath220 and @xmath2 mode is dominated by @xmath218 . by definition , in the case of @xmath221 , the compiled orfs are zero . we have studied the detectability of the stochastic gravitational waves with ptas . in most of the previous works , the isotropy of sgwb has been assumed for the analysis . recently , however , a stochastic gravitational wave background with anisotropy have been considered . the information of the anisotropic pattern of the distribution should contain important information of the sources such as supermassive black hole binaries and the sources in the early universe . it is also intriguing to take into account the polarization of sgwb in the pta analysis . therefore , we extended the correlation analysis to circularly polarized sgwb and calculated generalized overlap reduction functions for them . it turned out that the circular polarization can not be detected for an isotropic background . however , when the distribution has anisotropy , we have shown that there is a chance to observe circular polarizations in the sgwb . we also discussed how to separate polarized modes from unpolarized modes of gravitational waves . if we have a priori knowledge of the abundance ratio for each mode in each of @xmath205 , we can separate @xmath3 mode and @xmath2 mode in general . this would be possible if we start from fundamental theory and calculate the spectrum of sgwb . in particular , in the case that the signal of lowest @xmath222 is dominant , we performed the separation of @xmath3 mode and @xmath2 mode explicitly . this work was supported by grants - in - aid for scientific research ( c ) no.25400251 and "" mext grant - in - aid for scientific research on innovative areas no.26104708 and `` cosmic acceleration''(no.15h05895 ) . in this appendix , we perform angular integration of the generalized orf for dipole ( @xmath154 ) circular polarization ( see @xcite ) : @xmath223 where we have defined @xmath224 . it is obvious that in the case of @xmath225 , integrand of the generalized orf is zero , because of @xmath226 , then we obtain @xmath227 then , using - , we calculate @xmath228 and we find @xmath229 therefore we only have to consider the dipole generalized orf in the case of @xmath154 , @xmath230 : @xmath231 where @xmath232 first , to calculate @xmath233 , we use contour integral in the complex plane . defining @xmath234 and substituting @xmath235 into , we can rewrite @xmath233 as @xmath236 } \ , \end{aligned}\ ] ] where @xmath237 denotes a unit circle . we can factorize the denominator of the integrand and get @xmath238 where @xmath239 hereafter , the upper sign applies when @xmath240 and the lower one applies when @xmath241 . note that we only consider the region @xmath242 , so we have used the relation @xmath243 in above expression . in the region @xmath244 , @xmath245 is inside the unit circle @xmath237 except for @xmath246 and @xmath247 is outside the unit circle @xmath237 . now , we can perform the integral using the residue theorem @xmath248 where @xmath249 the residues inside the unit circle @xmath237 can be evaluated as @xmath250\right\ } = \frac{i(z_{+}+z_{-})}{2\sqrt{1-x^2}\sin\xi } \ , \end{aligned}\ ] ] @xmath251 thus , we obtain @xmath252 next , we consider @xmath253 defined in . using , we can calculate @xmath253 as @xmath254 similarly , we can evaluate @xmath255 given in . to calculate @xmath255 in the complex plane , we again substitute into and obtain @xmath256 we use the residue theorem @xmath257 where @xmath258 the residues inside the unit circle @xmath237 can be calculated as @xmath259\right\ } = \frac{i(z_{+}^2+z_{-}^2)}{4\sqrt{1-x^2}\sin\xi } \ , \end{aligned}\ ] ] @xmath260 therefore , @xmath255 becomes @xmath261 substituting to , we can calculate @xmath262 : @xmath263 finally , substituting and into , we get the generalized orf for @xmath264 @xmath265\ .\end{aligned}\ ] ] as a function of the angular separation between the two pulsars @xmath137 . [ gi](a ) shows monopole ( l=0 ) , fig . [ gi](b ) shows dipole ( l=1 ) , fig . [ gi](c ) shows quadrupole ( l=2 ) and fig . [ gi](d ) shows octupole ( l=3 ) . the black solid curve , the blue dashed curve , the dark - blue dash - dotted curve , the red dotted curve , the green long - dashed curve , the dark - green space - dashed curve represent @xmath149 , @xmath266 , @xmath267 , @xmath268 , @xmath269 , @xmath270 , respectively.,title=""fig:"",width=340 ] ( a ) @xmath152 as a function of the angular separation between the two pulsars @xmath137 . [ gi](a ) shows monopole ( l=0 ) , fig . [ gi](b ) shows dipole ( l=1 ) , fig . [ gi](c ) shows quadrupole ( l=2 ) and fig . [ gi](d ) shows octupole ( l=3 ) . the black solid curve , the blue dashed curve , the dark - blue dash - dotted curve , the red dotted curve , the green long - dashed curve , the dark - green space - dashed curve represent @xmath149 , @xmath266 , @xmath267 , @xmath268 , @xmath269 , @xmath270 , respectively.,title=""fig:"",width=340 ] ( b ) @xmath154 as a function of the angular separation between the two pulsars @xmath137 . fig . [ gi](a ) shows monopole ( l=0 ) , fig . [ gi](b ) shows dipole ( l=1 ) , fig . [ gi](c ) shows quadrupole ( l=2 ) and fig . [ gi](d ) shows octupole ( l=3 ) . the black solid curve , the blue dashed curve , the dark - blue dash - dotted curve , the red dotted curve , the green long - dashed curve , the dark - green space - dashed curve represent @xmath149 , @xmath266 , @xmath267 , @xmath268 , @xmath269 , @xmath270 , respectively.,title=""fig:"",width=340 ] ( c ) @xmath157 as a function of the angular separation between the two pulsars @xmath137 . [ gi](a ) shows monopole ( l=0 ) , fig . [ gi](b ) shows dipole ( l=1 ) , fig . [ gi](c ) shows quadrupole ( l=2 ) and fig . [ gi](d ) shows octupole ( l=3 ) . the black solid curve , the blue dashed curve , the dark - blue dash - dotted curve , the red dotted curve , the green long - dashed curve , the dark - green space - dashed curve represent @xmath149 , @xmath266 , @xmath267 , @xmath268 , @xmath269 , @xmath270 , respectively.,title=""fig:"",width=340 ] ( d ) @xmath159 in this appendix , we show orfs for the intensity @xcite . the following form for @xmath152 was derived in @xcite , and our expressions are identical to their expressions : @xmath271\ , \end{aligned}\ ] ] for , @xmath154 , we calculated as @xmath272\ , \\ \gamma^{i}_{11}&=&\frac{\sqrt{6\pi}}{12}\sin\xi\left[1 + 3(1-\cos\xi)\left\{1+\frac{4}{1+\cos","we study the detectability of circular polarization in a stochastic gravitational wave background from various sources such as supermassive black hole binaries , cosmic strings , and inflation in the early universe with pulsar timing arrays . we calculate generalized overlap reduction functions for the circularly polarized stochastic gravitational wave background . we find that the circular polarization can not be detected for an isotropic background . however , there is a chance to observe the circular polarization for an anisotropic gravitational wave background . we also show how to separate polarized gravitational waves from unpolarized gravitational waves ."
"as a common quantum phenomenon , the tunneling through a potential barrier plays a very important role in the microscopic world and has been studied extensively since the birth of quantum mechanics . one of the earliest applications of quantum tunneling is the explanation of @xmath0 decays in atomic nuclei . the quantum tunneling effect governs also many other nuclear processes such as fission and fusion . in particular , a lot of new features are revealed in sub - barrier fusion reactions which are closely connected with the tunneling phenomena @xcite . for most of the potential barriers , the penetrability can not be calculated analytically @xcite . among those potentials for which analytical solutions can be obtained , the parabolic potential @xcite is the mostly used in the study of nuclear fusion . by approximating the coulomb barrier to a parabola , wong derived an analytic expression for the fusion cross section @xcite which is widely adopted today in the study of heavy ion reactions ( see , e.g. , recent refs . the parabolic approximation works remarkably well both for the penetrability and for the fusion cross section at energies around or above the coulomb barrier @xcite . apparently the parabolic approximation breaks down at energies much smaller than the barrier height due to the long - range coulomb interaction . one may calculate the penetration probability numerically by using the path integral method or the wkb approximation . however , it is highly desirable to have an analytical expression for the barrier penetrability when one introduces an energy - dependent one - dimensional potential barrier @xcite or barrier distribution functions @xcite . in the present work , we derived a new barrier penetration formula based on the wkb approximation . the influence of the long coulomb tail in the barrier potential is taken into accout properly . therefore this formula is especially applicable to the barrier penetration with penetration energy much lower than the coulomb barrier . as a first attempt and a test study , we apply this new formula to evaluate @xmath0 decay half - lives of atomic nuclei . for the @xmath0 decay , the penetrability is usually calculated with the wkb approach @xcite , in other words , integrating numerically the wave number within two turning points at which the interaction potential is equal to the @xmath1-value of the @xmath0 decay . we will show that the present analytical formula reproduces the experimental results very well , especially for spherical nuclei . the paper is organized as follows . in sec . [ sec : formalism ] we present the new barrier penetration formula . the validity of the new formula is investigated and its application to @xmath0 decays are given in sec . [ sec : results ] . finally in sec . [ sec : summary ] we summarize our work . in the appendix , the detailed derivation of the new penetration formula is given . when the penetration energy is well below the coulomb barrier , the barrier penetrability formula derived from the wkb approximation reads , @xmath2 , \label{eq : wkb}\ ] ] where the potential usually consists of three parts , the nuclear , the coulomb , and the centrifugal potentials , @xmath3 @xmath4 and @xmath5 are the inner and outer turning points determined by the relation @xmath6 . by approximating @xmath7 to a parabola with the height @xmath8 and the width @xmath9 , eq . ( [ eq : wkb ] ) is reduced as @xmath10 , \label{eq : hw}\ ] ] which has been widely used in the study of heavy ion reactions . because of the long - range coulomb interaction , the coulomb barrier given in eq . ( [ eq : potential ] ) has a long tail and is asymmetric . thus for the penetration well below the barrier , the parabolic approximation is not valid . we may divide the potential barrier into two parts at the barrier position @xmath11 . the first part of @xmath7 with @xmath12 could still be approximated by half of a parabola and we need to evaluate the integration in eq . ( [ eq : wkb ] ) in the range @xmath13 only . for s wave , the integral in eq . ( [ eq : wkb ] ) is evaluated as , @xmath14 , \label{eq : x1x2}\ ] ] with @xmath15 under the parabolic approximation and @xmath16 \nonumber \\ & & \mbox { } + \frac { k a } { \sqrt { \tau - 1 } } \frac{v_0}{e } \ln [ 1 + e^ { ( r_0 - r_b ) / a } ] \label{eq : new } , \end{aligned}\ ] ] where @xmath17 and @xmath18 . the details of the derivation of eq . ( [ eq : new ] ) are given in the appendix . it should be mentioned that in the derivation of eq . ( [ eq : new ] ) , a woods - saxon form is used for @xmath19 . in this section , we use the new formula to study the typical barrier penetration problem , @xmath0 decays of atomic nuclei . the @xmath0 decay half - life is related to the decay width @xmath20 by @xcite @xmath21 the decay width @xmath20 is calculated as @xcite @xmath22 where @xmath23 is the assaults frequency of @xmath0 particle on the barrier , @xmath24 the spectroscopic or preformation factor and @xmath25 the penetrability with @xmath1 the @xmath0 decay q - value . for spherical nuclei , @xmath26 is parametrized as @xcite @xmath27 and the penetrability will be calculated with eqs . ( [ eq : x1x2 ] ) , ( [ eq : left ] ) , and ( [ eq : new ] ) . ( color online ) the barrier potential between the @xmath0 and the daughter nucleus for @xmath28po and @xmath29nd . the solid curve shows the exact potential @xmath7 and the dashed curve stands for the effective potential given in eq . ( [ eq : veff ] ) associated with the parabolic approximation eq . ( [ eq : left ] ) and the new barrier penetration formula eq . ( [ eq : new ] ) . note that the two curves are almost identical to each other . , title=""fig:"",scaledwidth=45.0% ] ( color online ) the barrier potential between the @xmath0 and the daughter nucleus for @xmath28po and @xmath29nd . the solid curve shows the exact potential @xmath7 and the dashed curve stands for the effective potential given in eq . ( [ eq : veff ] ) associated with the parabolic approximation eq . ( [ eq : left ] ) and the new barrier penetration formula eq . ( [ eq : new ] ) . note that the two curves are almost identical to each other . , title=""fig:"",scaledwidth=45.0% ] for the @xmath0-nuclear interaction , we adopt the coulomb and the woods - saxon potentials and parameters proposed in ref . @xcite , @xmath30 , & r \le r_m , \end{cases } \label{eq : coulomb}\ ] ] and @xmath31 } , \end{aligned}\ ] ] with @xmath32 and @xmath33 the mass and charge numbers of the daughter nucleus and @xmath1 the @xmath0 decay energy . the parameters in these potentials and given in eq . ( [ eq : prefomation ] ) were obtained by fitting @xmath0 decay half lives and cross section data for several fusion reactions . it can be easily verified that the position of the coulomb barrier @xmath11 is larger than @xmath34 thus the use of the coulomb force given in eq . ( [ eq : coul ] ) is valid . before the new formula is used to study alpha decays , we investigate in details its validity . first we examine how the effective potential connected with the new formula eq . ( [ eq : new ] ) is close to the exact one . two extreme examples are chosen for this purpose , @xmath28po which has a very short half - life @xmath35 s and @xmath29nd which has a quite long half - life @xmath36 s @xcite . the barrier potential @xmath7 is shown in fig . [ fig : potential ] for these two systems . the effective potential , @xmath37 is also shown for comparison . @xmath38 fm is the radial position outside of which the nuclear part of the @xmath0-nucleus potential could be neglected ( see the appendix for more details ) . in our calculations , the width of the parabolic potential is obtained by fitting the barrier potential from the inner turning point @xmath39 to the position of the barrier @xmath11 . unlike the full parabolic approximation , the effective potential is asymmetric and coincides with the exact potential very well , especially the outer side of the barrier which critically influences @xmath0 decays . .[tab : deviation ] comparison of the results for the barrier penetration probability for @xmath0 decays in po isotopes ( charge and mass numbers of the @xmath0 emitter are listed in the first and the second entries ) . the meaning of @xmath40 is given in eq . ( [ eq : x1x2 ] ) . the superscript `` wkb '' means the penetrability calculated from the wkb approach , `` para '' from the parabolic approximation in eq . ( [ eq : left ] ) , and `` new '' from the new formulas eq . ( [ eq : new ] ) . [ cols=""^,^,^,^,^,^,^"",options=""header "" , ] the deformation influences the @xmath0 decay life time both on the preformation mechanism and on the penetration process @xcite . in the present work , we have assumed the barrier potential to be spherical . in 68 of these 344 nuclei , the spherical potential assumption is met well ( with @xmath41 for the daughter nucleus @xcite ) . in table [ tab : spherical ] the calculated and experimental values of the @xmath0 decay half lives for these nuclei are given . the statistical summary is also shown in the last line of table [ tab : stati ] . it is found that the new formula gives very good results for these spherical nuclei . in most cases , the differences between the calculated and the experimental values of @xmath42 are smaller than 0.5 . the root mean square deviation between @xmath43 $ ] and @xmath44 $ ] is 0.34 . in the study of barrier penetration in nuclear physics , the parabolic approximation is usually adopted because an analytical solution exists for the penetrability of a parabola barrier potential . the parabola approximation works indeed well both for the penetrability and for the fusion cross section at energies around or above the coulomb barrier . but it fails at energies much smaller than the barrier height due to the long - range coulomb interaction . in the present work , we derived a new barrier penetration formula , eq . ( [ eq : new ] ) , based on the wkb approximation . we took into account the influence of the long coulomb tail in the barrier potential properly . therefore this formula is especially applicable to the barrier penetration with penetration energy much lower than the coulomb barrier . we have shown that the present analytical formula reproduces the wkb results very well . this new penetration formula is used to calculate @xmath0 decay half - lives of 344 nuclei with the @xmath0-nucleus potential given in ref . satisfactory agreement between the present calculation and the experiment is achieved . for spherical and even - even nuclei , the results are particularly good . therefore , the new formula could be used in the study of barrier penetration at energies much smaller than the barrier height . furthermore , we expect that the new formula will facilitate the study of the barrier penetrability where one has to introduce an energy - dependent one - dimensional potential barrier or a barrier distribution function . this work was partly supported by the national natural science foundation ( 10575036 , 10705014 , and 10875157 ) , the major state basic research development program of china ( 2007cb815000 ) , the knowledge innovation project of cas ( kjcx3-syw - n02 and kjcx2-sw - n17 ) , and deutsche forschungsgemeinschaft . the computation of this work was supported by supercomputing center , cnic , cas . in order to evaluate the integration @xmath45 in eq . ( [ eq : x1x2 ] ) , we divide the potential between the position of the barrier @xmath11 and the outer turning point @xmath5 into two parts , @xmath46 and @xmath47 . @xmath48 should be large enough so that the nuclear potential vanishes for @xmath49 . for s wave , @xmath50 it has been verified that when @xmath48 is not very close to @xmath5 , @xmath51 , therefore , @xmath52\ dr \nonumber \\ & & \mbox { } + 2 \int^{r _ \mathrm{out } } _ { r_\mathrm{v } } \sqrt { \frac{2\mu } { \hbar^2 } \left ( v_\mathrm{c}(r ) - e \right ) } \ dr \nonumber \end{aligned}\ ] ] @xmath53 since the coulomb potential outside the barrier ( @xmath54 ) is well described by [ c.f . ( [ eq : coulomb ] ] , @xmath55 the first term in the above equation can be evaluated easily as , @xmath56 , \end{aligned}\ ] ] with @xmath17 and @xmath18 . for the evaluation of the second term in eq . ( [ eq : x1x2_approx ] ) , we adopt a woods - saxon form for the nuclear part of the barrier potential , @xmath57 } , \end{aligned}\ ] ] and replace @xmath58 in the denominator by @xmath59 , @xmath60 \right\ } \right|_{r_b}^ { r _ { \mathrm{v } } } \nonumber \\ & \approx & \frac { k } { \sqrt { \tau - 1 } } \frac{v_0}{e } \ { r_0- r_b + a \ln [ 1 + e^ { ( r_b - r_0 ) / a } ] \ } \nonumber \\ & = & \frac { k a } { \sqrt { \tau - 1 } } \frac{v_0}{e } \ln [ 1 + e^ { ( r_0 - r_b ) / a } ] .\end{aligned}\ ] ] in the above derivation , we have used the fact that @xmath61 \gg 1 $ ] for @xmath0 decay and penetration well below the coulomb barrier . finally , we have an analytical expression for @xmath45 , @xmath62 \nonumber \\ & & \mbox { } + \frac { k a } { \sqrt { \tau - 1 } } \frac{v_0}{e } \ln [ 1 + e^ { ( r_0 - r_b ) / a } ] .\end{aligned}\ ] ]","starting from the wkb approximation , a new barrier penetration formula is proposed for potential barriers containing a long - range coulomb interaction . this formula is especially proper for the barrier penetration with penetration energy much lower than the coulomb barrier . the penetrabilities calculated from the new formula agree well with the results from the wkb method . as a first attempt , this new formula is used to evaluate @xmath0 decay half - lives of atomic nuclei and a good agreement with the experiment is obtained ."
"for the hybrid monte carlo algorithm ( hmc)@xcite , often used to study quantum chromodynamics ( qcd ) on the lattice , one is interested in efficient numerical time integration schemes which are optimal in terms of computational costs per trajectory for a given acceptance rate . high order numerical methods allow the use of larger step sizes , but demand a larger computational effort per step ; low order schemes do not require such large computational costs per step , but need more steps per trajectory . so there is a need to balance these opposing effects . omelyan integration schemes @xcite of a force - gradient type have proved to be an efficient choice , since it is easy to obtain higher order schemes that demand a small additional computational effort . these schemes use higher - order information from force - gradient terms to both increase the convergence of the method and decrease the size of the leading error coefficient . other ideas to achieve better efficiency for numerical time integrators are given by multirate or nested approaches . these schemes do not increase the order but reduce the computational costs per path by recognizing the different dynamical time - scales generated by different parts of the action . slow forces , which are usually expensive to evaluate , need only to be sampled at low frequency while fast forces which are usually cheap to evaluate need to be sampled at a high frequency . a natural way to inherit the advantages from both force - gradient type schemes and multirate approaches would be to combine these two ideas . previously , we studied the behavior of the adapted nested force - gradient scheme for the example of the @xmath0-body problem @xcite and would like to learn more about their usefulness for lattice field theory calculations . due to the huge computational effort required for qcd simulations , it is natural to attempt an intermediate step first . we chose the model problem of quantum electrodynamics ( qed ) in two dimensions , the schwinger model @xcite , since it is well - suited as a test case for new concepts and ideas which can be subsequently applied to more computationally demanding problems @xcite . as a lattice quantum field theory , it has many of the properties of more sophisticated models such as qcd , for example the numerical cost is still dominated by the fermion part of the action . the fact that this model , with far fewer degrees of freedom , does not require such large computational effort makes it the perfect choice for testing purposes . we compare the behavior of numerical time integration schemes currently used for hmc @xcite with the nested force - gradient integrator @xcite and the adapted version introduced in @xcite . we investigate the computational costs needed to perform numerical calculations , as well as the effort required to achieve a satisfactory acceptance rate during the hmc evolution . our goal is to find a numerical scheme for the hmc algorithm which would provide a sufficiently high acceptance rate while not drastically increasing the simulation time . the paper is organized as follows . in section 2 we give a short overview of the hmc algorithm and numerical schemes for time integration , which are used in hmc . in section 3 we present the 2-dimensional schwinger model and introduce the idea of the force - gradient approach and the resulting novel class of numerical schemes . section 4 is devoted to the results of a comparison between widely used algorithms and the new approach and section 5 draws our conclusion . in this section we provide a general overview of the hmc algorithm @xcite to introduce our novel integrator . we also present some standard numerical time integrating methods used in hmc , as well state - of - the - art numerical schemes , which we later compare by applying them to the two - dimensional schwinger model . in the hybrid monte carlo algorithm , the quantum lattice field theory is embedded in a higher - dimensional classical system through the introduction of a fictitious ( simulation ) time @xcite . the gauge field @xmath1 is associated with its ( fictitious ) conjugate momenta @xmath2 , and the classical system is described by the hamiltonian , @xmath3 + { \mathcal{b}}[u],\ ] ] where @xmath4 $ ] and @xmath5 $ ] represent the kinetic and potential energy respectively . for a given configuration @xmath1 , a new configuration @xmath6 is generated by performing an hmc update @xmath7 , which consists of two steps : * * molecular dynamics trajectory : * evolve the gauge fields @xmath1 , elements of a lie group , and the momenta @xmath2 , elements of the corresponding lie algebra , in a fictitious computer time @xmath8 according to hamilton s equations of motions @xmath9 since analytical solutions are not available in general , numerical methods must be used to solve the system of eqn . . the discrete updates of @xmath1 and @xmath2 with an integration step @xmath10 are @xmath11 leading to a first - order approximation at time @xmath12 . since the momenta @xmath2 are elements of lie algebra , we have an additive update of @xmath2 . on the other hand , the links @xmath1 must be elements of the lie group , therefore an exponential update is used for @xmath1 to preserve the underlying group structure . * * metropolis step : * accept or reject the new configuration @xmath13 with probability @xmath14 where @xmath15 . in this paper we are concerned with numerical time integration schemes , which preserve the fundamental properties of geometric integration , time - reversibility and volume - preservation . all numerical schemes presented below possess these necessary properties . * basic schemes : * well - known , commonly used integration schemes in molecular dynamics are given by * the leap - frog method , a 3-stage composition scheme of the discrete updates defined above : @xmath16 * and a 5-stage extension widely used in qcd computations : @xmath17 * force gradient schemes : * force - gradient schemes increase accuracy by using additional information from the force gradient term @xmath18 , with @xmath19 defining lie brackets . the 5-stage force - gradient scheme proposed by omelyan et al @xcite is the simplest ; @xmath20 here we also test the modification of the force - gradient method proposed in @xcite , where the force - gradient term @xmath21 is approximated via a taylor expansion . an extension is given by the 11-stage decomposition @xcite , recently implemented as the integrator in the open source code openqcd as one of the standard options @xcite @xmath22where @xmath23 , @xmath24 , @xmath25 and @xmath26 are parameters from equation ( 71 ) in ref . @xcite . * nested schemes : * qed and qcd problems usually lead to hamiltonians with the following fine structure @xmath27 + { \mathcal{b}}_{1}[u]+ { \mathcal{b}}_{2}[u],\ ] ] where the action of the system can be split into two parts : a fast action @xmath28 such as the gauge action , and a slow part @xmath29 , for example , the fermion action . this allows us to apply the idea of multirate schemes ( an idea known as nested integration in physics literature)@xcite in order to reduce the computational effort . at first we consider the nested version of the leap - frog method @xmath30 where the inner cheaper system @xmath31+{\mathcal{b}}_{1}[u]$ ] is solved by @xmath32 with @xmath33 being a number of iterations for the fast part of the action . our main goal is to compare the above - mentioned methods with more elaborated nested schemes : in @xcite , a similar 5-stage decomposition scheme has been recently introduced : @xmath34 a nested version of , which has been used in @xcite reads @xmath35 where @xmath36 with @xmath37 and @xmath38 . in the limit @xmath39 we have @xmath40 . note that this approach uses force - gradient information at all levels , i.e. , the high computational cost of high order schemes appears at all levels . one may overcome this problem by using schemes of different order at the different levels without losing the effective high order of the overall multirate scheme . for the latter , we include appropriate force gradient information as we explain in the following for the case of two time levels , where the gauge action plays the role of the fast and cheap part , and the fermionic action plays the role of the slow and expensive part . the reasoning is as follows : if one uses the 5-stage sexton - weingarten integrator of second order for the slow action , and approximates the fast action by @xmath41 leap - frog steps of step size @xmath42 , the error of the overall multirate scheme will be of order @xmath43 . with the use of force gradient information only at the slowest level it is possible to cancel the leading error term of order @xmath44 . as @xmath45 usually holds in the multirate setting , the overall order is then given by the leading error term of order @xmath46 , i.e. , the scheme has an effective order of four . one example for such a scheme for problems of type is given by the 5-stage nested force - gradient scheme introduced in @xcite @xmath47 to summarize , the adapted scheme differs from the original one in two perspectives : * the force gradient scheme for the fast action is replaced by a leap - frog scheme . * only the part @xmath48 of the full force gradient @xmath49 is needed to gain the effective order of four . the numerical schemes - and - are second order convergent schemes . methods - and - have the fourth order of convergence . we do not consider integrators of higher order than four since the computational costs are too high . the schemes of the same convergence order differ from each other by the number of stages ( updates of momenta and links per time step ) . usually methods with more stages have smaller leading error coefficients and therefore have better accuracy , but higher computational costs . we would like to determine which integrator would represent the best compromise between high accuracy and computational efficiency . we will apply all these numerical integration schemes to the two - dimensional schwinger model . the most challenging task from the theoretical point of view is to derive the force - gradient term @xmath21 . in the next section we introduce the schwinger model and explain how to obtain the force - gradient term . the 2 dimensional schwinger model is defined by the following hamiltonian function @xmath50 = \frac{1}{2}\sum_{n=1,\mu=1}^{v,2 } p_{n,\mu}^2 + s_g[u ] + s_f[u].}\ ] ] with @xmath51 the volume of the lattice . unlike qcd , where @xmath52 and @xmath53 , for this qed problem , the links @xmath1 are the elements of the lie group @xmath54 and the momenta @xmath55 belong to @xmath56 , which represents the lie algebra of the group @xmath54 . this makes this test example very cheap in terms of the computational time . this together with the fact that the schwinger model also shares many of the features of qcd simulations , makes the schwinger model an excellent test example when considering numerical integrators : a fast dynamics given by the computationally cheap gauge part @xmath57 $ ] of the action demanding small step sizes , and a slow dynamics given by the computationally expensive fermion part @xmath58 $ ] allowing large step sizes . the pure gauge part of the action @xmath59 sums up over all plaquettes @xmath60 in the two - dimensional lattice with @xmath61 and is given by @xmath62 the links @xmath1 can be written in the form @xmath63 and connect the sites @xmath0 and @xmath64 on the lattice ; @xmath65 $ ] , @xmath66 , @xmath67 @xmath68 are respectively space and time directions and @xmath69 is a coupling constant . note that from now on we will set the lattice spacing @xmath70 . the fermion part of the action @xmath71 is given by @xmath72 where @xmath26 is a complex pseudofermion field . here , @xmath73 denotes the wilson dirac operator given by @xmath74 where @xmath75 are the pauli matrices @xmath76 @xmath77 is the mass parameter and the kronecker delta @xmath78 acts on the pseudofermion field by @xmath79 with @xmath80 the pseudofermion field , a vector in the two - dimensional spinor space taking values at each lattice point @xmath0 . in order to proceed with the numerical integration we need to obtain the force @xmath81 and the force gradient term @xmath21 . the force term @xmath82 with respect to the link @xmath83 is given by the first derivative of the action @xmath84 and can be written as @xmath85 since the numerical schemes use the multi - rate approach , the shifts in the momenta updates are split on @xmath86 and @xmath87 and we can consider them separately . the force terms @xmath86 and @xmath87 are obtained by differentiation over @xmath54 group elements , which for the schwinger model is the standard differentiation . the force associated with link @xmath88 from the gauge action is given by @xmath89 the force term of the fermion part is given by @xmath90 \,\ ] ] where vectors @xmath91 and @xmath92 are given @xmath93 for the numerical methods and we need to find the force gradient term @xmath94 with respect to the link @xmath83 . in case of the schwinger model this term reads @xmath95 for simplicity we decompose the force gradient term in four parts @xmath96 this decomposition is also useful since the numerical integrator only uses the term @xmath97 by construction . as shown in @xcite , to obtain the fourth order convergent scheme from the second order convergent method we must eliminate the leading error term , which is exactly represented by @xmath97 . for completeness we discuss all 4 parts below . the @xmath98 part of the force - gradient term is @xmath99 \end{aligned}\ ] ] with the set of plaquettes @xmath100 then by using the vectors @xmath101 defined in we obtain the @xmath102 piece of the force - gradient term given by @xmath103 . } \end{aligned}\ ] ] the second derivative of the fermion action is @xmath104 \xi + } \nonumber \\ & & { 2 \operatorname{re } \chi^\dagger \frac{\partial d}{\partial q_\mu(n ) } ( d^\dagger d)^{-1 } \frac{\partial d^\dagger}{\partial q_\nu(m ) } \chi \ , , } \nonumber \\ & & { = 2 \operatorname{re } \left [ z_{1,m,\nu}^\dagger \frac{\partial d}{\partial q_{\mu}(n ) } \xi + \chi^\dagger \frac{\partial d}{\partial q_{\mu}(n ) } d^{-1 } w_{2,m,\nu } - \chi^\dagger \frac{\partial^2 d}{\partial q_{\nu}(m ) \partial q_{\mu}(n ) } \xi + \chi^\dagger \frac{\partial d}{\partial q_{\mu}(n ) } d^{-1 } z_{1,m,\nu}\right ] } \nonumber \\ & & { = 2 \textrm{re } \left [ z_{1,m,\nu}^\dagger w_{2,n,\mu } + w_{1,n,\mu}^\dagger z_{2,m,\nu } - \chi^\dagger \frac{\partial^2 d}{\partial q_{\nu}(m ) \partial q_{\mu}(n ) } \xi \right ] } \end{aligned}\ ] ] in terms of the vectors @xmath105 and @xmath92 defined in . now the fields @xmath106 and @xmath107 are given by @xmath108 with @xmath109 in order to calculate @xmath110 and @xmath111 it is possible to perform the summation of @xmath112 before the inversions of @xmath73 and @xmath113 to get @xmath114 and @xmath115 which save @xmath116 additional inversions for the force gradient terms . it follows for the force gradient term @xmath111 @xmath117\ ] ] with @xmath118 + z_1 \right ) \ , . \end{aligned}\ ] ] the expression for @xmath110 can be obtained from the one for @xmath111 by replacing in and the vector @xmath119 with @xmath120 defined in . it is important to mention that the computationally most demanding part of the numerical integration of the schwinger model and quantum field theory in general is the inverse of the dirac operator @xmath121 . every momenta update , which includes fermion action requires 2 inversions of the dirac operator , the addition of the force - gradient term @xmath21 requires 4 more inversions . therefore leap - frog based methods and need 4 computations of @xmath121 per time step ; schemes and 6 times ; force - gradient based methods 8 for and , 10 for and the 11 stage method has 12 inversions of the dirac operator . since we use the multi - rate approach for schemes , and , which leads generally to fewer macro time steps needed than for the standard schemes we expect the integrator will be the most efficient choice among the methods considered . in the next section we present numerical tests of this prediction . in this section we apply the numerical integrators to compute the molecular dynamics step for the schwinger model when studied with the hmc algorithm . we consider a @xmath122 by @xmath122 lattice with a coupling constant @xmath123 and mass @xmath124 . the parameters were taken from @xcite and correspond to the scaling variable @xmath125 defined in @xcite.we have chosen them to simulate close to the scaling limit with light fermions and also to increase the impact of the fermion part of the action . we use one thermalised gauge configuration . for each integrator and value of the step - size we generate @xmath126 independent sets of momenta and integrate the equations of motion on a trajectory of length @xmath127 . we compute the absolute error @xmath128 and estimate its statistical error from the standard deviation . also the parameter @xmath33 is chosen in such a way to make micro step size to be @xmath129 times smaller than the macro step size @xmath10 . figure [ fig:1 ] presents the comparison between the numerical integrators . it shows the absolute error @xmath128 versus the step - size of the numerical scheme . here the multi - rate schemes , , and outperform their standard versions as expected . also it is easy to see that the scheme has the best accuracy and the nested force - gradient method just slightly edges the adapted nested force - gradient scheme . figure [ fig:2 ] presents the cpu time , required for the proposed integrators , versus the achieved accuracy . we can observe that the nested force - gradient method and adapted nested force- gradient method show much better results in terms of a computational efficiency than the integrators and ; and even compared to the 11 stage scheme . here we can see that the modification of proposed in @xcite also performs better than its original version . it shows almost similar computational costs as nested versions of the force - gradient approach - , since it has the same number of @xmath121 ( see table [ tab:1 ] ) . but it is less efficient because the proposed nested approach is more precise . .step - sizes and number of inversions of @xmath73 per step and per trajectory for acceptance rate of 90% [ cols=""^,^,^,^,^"",options=""header "" , ] table [ tab:1 ] shows the number of inversions of the dirac operator @xmath73 , which is needed to reach 90% acceptance rate of the hmc . since @xmath121 is the most computationally demanding part it is important to see how many of these inversions are required per each trajectory . from table [ tab:1 ] it easy to see that the adapted nested force - gradient method and nested force - gradient method need the least number of @xmath121 per trajectory to reach the chosen acceptance rate @xmath130 . we can also claim that methods and have a potential to perform even better with respect to the computational effort in the case of lattice qcd problems , since the impact of the fermion action and the computational time to obtain the inversion of the dirac operator @xmath73 is much more significant . we presented the nested force - gradient approach and its adapted version applied to a model problem in quantum field theory , the two - dimensional schwinger model . the derivation of the force - gradient terms was given and the schwinger model was introduced . nested force - gradient schemes seem to be an optimal choice with relatively high convergence order and low computational effort . also it would be possible to improve the algorithm by measuring the poisson brackets of the shadow hamiltonian of the proposed integrator and then tuning the set of optimal parameters , e. g. micro and macro step sizes . + in future work we will apply this approach to the hmc algorithm for numerical integration in lattice qcd . here we expect the adapted nested - force gradient scheme to outperform the original one , if we further partition the action into more than two parts , by using techniques to factorize the fermion determinant : less force - gradient information is needed for the most expensive action , and only leap - frog steps are needed for the high frequency parts of the action . this work is part of project b5 within the sfb / transregio 55 _ hadronenphysik mit gitter - qcd _ funded by dfg ( deutsche forschungsgemeinschaft ) . s. duane , a.d . kennedy , b.j . pendleton , d. roweth , hybrid monte carlo , phys . b195 ( 1987 ) , pp . e. hairer , c. lubich , g. wanner , geometric numerical integration : structure - preserving algorithms for ordinary differential equations , springer , berlin , 2002 . omelyan , i.m . mryglod , r. folk , symplectic analytically integrable decomposition algorithms : classification , derivation , and application to molecular dynamics , quantum and celestial mechanics , comput . 151 ( 2003 ) , pp .","we study a novel class of numerical integrators , the adapted nested force - gradient schemes , used within the molecular dynamics step of the hybrid monte carlo ( hmc ) algorithm . we test these methods in the schwinger model on the lattice , a well known benchmark problem . we derive the analytical basis of nested force - gradient type methods and demonstrate the advantage of the proposed approach , namely reduced computational costs compared with other numerical integration schemes in hmc ."
"one of the main goals of the search for periodic isolated sources of gravitational waves ( g.w . ) is to perform all sky surveys , based on `` blind searches '' , where the source parameters are unknown . in this case hierarchical procedures are applied , based on a sequence of increasing resolution steps . in this paper we study in details the problem of sensitivity loss due to discretization of parameters and to the needs to limit the computing cost , with hough procedures . in particular , we propose and study the characteristics of a frequency hough procedure , designed mainly to reduce the discretization problem , and we compare it with the sky hough procedure , which is actually used in the virgo collaboration . + the paper is organized as follows : in sect . 2 we present the basic scheme of the rome hierarchical procedure , based on the main idea of coincidences among subsets of data ; in sect . 3 we discuss the limits due to digitization of the sky hough procedure ; in sects . 4 , 5 we present the new frequency hough procedure , discussing details its implementation and its basic characteristics ; in sect . 6 we present the study of amplitude losses due to digitization , and thus efficiencies , for both the procedures . conclusions and comments are given in sect . hierarchical procedures , based on hough transform algorithms , are applied by various groups in the g.w . community . see , for example , references @xcite . there are various ways of implementing the hierarchical procedure and the hough transform . the hough transform is a linear transform that is used to recognize the parameters of the analytical description of a curve from the position of some points on it . it operates on an `` image '' of points , in our case the peakmap in the time - frequency plane . for each peak of this map we increase a set of bins of a multi - dimensional histogram ( in our case a two - dimensional histogram ) defined on the parameters space , called the hough map . in the old procedure , the parameter space was the position of the source , i.e. the celestial sphere , and we fixed the spin down value for each hough map . in the new one , the parameter space is the plane @xmath0 , and for each hough map , we fix the position of the source . the mapping ( i.e. which points of the hough map must be increased for a certain point in the peakmap ) can be done in different ways : we use always what we call the `` biunivocal mapping '' , i.e. a mapping in which every point in the hough map derive from a single point of the peakmap at a given time . it is easy to demonstrate that in this case the mapping is also uniform , i.e. in the case of uniformly distributed random dots in the peakmap , the expected value of the hough map @xmath1 is a constant ( for all parameter value ) . this value , depending on the number n of the spectra of the peakmap and on the mapping , defines the `` noise '' of the map . it is binomially distributed with parameters n and @xmath2 . we will refer here to the rome scheme , presently used in virgo data . [ fig : schema ] shows the basic scheme of the rome hierarchical procedure . details on the main aspects of the procedure are given in references @xcite . after data cleaning ( short time domain disturbances removal ) and `` short ffts data base '' ( sfdb ) creation , peakmaps are computed , using a very refined auto - regressive algorithm to equalize the spectral data by an appropriate follow - up of the noise . peakmaps are frequency vs time maps , obtained from equalized spectra by selecting all the local maxima above a chosen threshold . an accurate cleaning of peakmaps , by removing known noise lines and the more persistent lines , is needed and its implementation is critical for the next step analysis . on the cleaned peakmaps , methods of peaks detection are applied . that is , transformation from the input plane to the hough plane , thresholding and first order candidates selection . candidate parameters are defined by source frequency , celestial coordinates , first spin - down parameter . the need for coincidences among candidates obtained in different subsets of data ( two in the scheme of fig . [ fig : schema ] ) has been discussed in references @xcite . this method is very efficient to reduce the number of spurious candidates at a fixed threshold . thus , for a given false alarm probability , we can lower the threshold -with respect to the choice of not doing coincidences- gaining in detection efficiency . the method has a better efficiency when the data sets have similar sensitivities . after the coincidence , the survived candidates are analyzed coherently with longer ffts on corrected data . then the spectral filtering is used to take into account the spread of the power in five bands , as explained in reference @xcite . finally , second order candidates are produced . as stated before , the sky hough method shows amplitude losses , and thus loss of sensitivity , which are due to digitization of parameters . this effect shows up mainly for the complexity of the transform together with the need of reducing the computing cost : * the method is based on a transform between the time - frequency peakmap and the celestial sphere . it is not simple for the non linearity of the mapping ; * to reduce the computational effort , we need to use `` look - up tables '' which introduce further digitization errors ; * to reduce the computational effort , fast algorithms have been developed , which require the use of a rectangular grid to map the sky . compared to the `` optimal '' ( see later ) grid , the rectangular one has over - resolution in some regions of the sky . this leads also to a higher number of candidates . * the use of the celestial map as the space to spot the candidates is very prone to artifacts , see @xcite : some regions are always `` privileged '' , that is they have a higher candidates number with respect to the expectation . the problem arises because each hough map is constructed over the whole sky . hence , it seemed important the study of alternative procedures . given the observation that most of the problems are related to the complexity of the transformation , we exploit the possibility of the use of a different but simpler transformation . a part the simplicity of the transformation we obviously need to study a procedure which is less , or equivalently , computationally expensive . therefore we studied a procedure which has a better , or equivalent , sensitivity , at the same computational cost of the sky hough . the transformation we propose transforms the * time - observed frequency * plane into the * source frequency - spin down * plane . let s go into details . if @xmath3 is the frequency ( doppler corrected for a given sky direction ) , @xmath4 the source intrinsic frequency , @xmath5 the first spin - down parameter , @xmath6 the time at the detector and @xmath7 a reference time , we have that @xmath8 a straight line in the hough plane . we then get the following : @xmath9 each point in the input plane @xmath10 , that is a peak in the doppler shifted peakmap , is transformed into a straight line in the hough @xmath11 plane , with slope @xmath12 . the slope depends on the choice of the reference time . if we choose @xmath7 equal to the beginning time of the data we analyze , then the slope is always negative and inversely proportional to the time gap . + this is the choice we have done here . in addition , considering the width @xmath13 of the frequency bins in the input plane we notice that each peak is transformed into a stripe among two parallel straight lines @xmath14 it is a linear transformation . now the input plane is obtained from the original peakmap by correcting it for the doppler shift due to the earth revolution and rotation , for each point in the sky grid we need to analyze . thus `` time '' is the time at the detector and `` frequency '' the observed frequency , after the doppler correction . but , as each sfdb is short enough to not be affected by a time - varying doppler shift , then the doppler effect removal from the original peakmap , obtained from the collection of all the sfdb data , reduces to a very simple `` shifting '' procedure of the peakmap bins . in the analysis scheme , this bins shift is part of the hough procedure . + in the following , we give details on the construction of the map . the frequency hough map is constructed using the `` direct differential method '' , as is done with the sky hough . with this method , instead of building directly the hough map , one builds a map that , if `` integrated '' ( i.e. summed over bins from left to right ) , gives the hough map . this is important to minimize the number of floating point operations . as already explained , for each sky position , the input peakmap is got from the original one by shifting bins to correct for the doppler effect . the sky is sampled with a non uniform covering grid , which will be later discussed . here we explain in detail the technique , by giving the sequence of operations : * for each point in the sky grid , for each coordinate in the input plane @xmath10 and for each spin - down value @xmath15 , the map is incremented by 1 in the point @xmath16 and decremented by 1 in the point @xmath17 . hence , for each sky position , a differential map is constructed . the sum of the bins along the frequency direction is then performed to construct the final integral map . this two dimensional histogram is the frequency hough map . in the algorithm implementation we plan to divide the input peakmap into 10 hz bands , thus constructing , for each position in the sky , a different hough map every 10 hz . + in case there is the need to exploit higher order one spin down parameters , one ( or more ) loop(s ) has ( have ) to be added to the sequence of operations , to scan the discrete set of values of the new parameter(s ) . this clearly influences the computing cost , but does not change the basics of the method . let s first discuss two peculiar aspects of this new method , which are the basis of its appeal . from the given analysis scheme , it is easy to see that the frequency resolution for the estimation of the source frequency @xmath4 can be enhanced , with respect to the binning frequency @xmath13 , without relevantly affecting the computational effort . in fact , the use of a resolution @xmath18 with @xmath19 , affects only the size of the hough map . this has a computational cost only when summing over the bins to construct the integral map from the differential one . but we notice that the total cost of the construction of the hough map is due to the construction of the differential map , dominated by the number of peaks in the peakmap and to the construction of the integral map , dominated by the number of bins . the former , in all practical cases , is the one which dominates . + the possibility to enhance the frequency resolution results to be , as will be shown in the next sections , a very important peculiarity of the new method . it which enhances considerably the efficiency , by reducing the digitalization effect . the same in the sky hough procedure would have a relevant computational cost . regarding the increasing of the spin down resolution , it would cost for both the procedures : the better the resolution in the spin down estimation the higher is the number of loops of the procedures . here we describe how we construct the grid on the sky . suppose two sources , at the same frequency @xmath4 and same latitude @xmath20 . their angular delay @xmath21 with respect to the detector rotation produces a time delay @xmath22 . the two sources will then have the same frequency variation at the detector , which is the classical equation due to the doppler effect , @xmath23 but with time delay @xmath24 . the observed frequency difference has thus a maximum value which is given by @xmath25 thus the angular resolution is , in radians : @xmath26 where @xmath27 is the number of points in the doppler band for a signal of max frequency @xmath4 : @xmath28 and @xmath29 . + we now repeat the same reasoning , supposing the two sources , at the same frequency @xmath4 and same longitude @xmath30 . the two sources will have the same frequency variation at the detector , now given by @xmath31 , but with an angular delay @xmath32 . the observed frequency difference has a maximum value which is : @xmath33 we obtain for the angular resolution , in radians : @xmath34 using eqs . [ gammalong ] and [ gammalat ] we get : @xmath35 @xmath36 using these equations we construct the grid on the sky , which we call the `` optimal '' grid . the points of the grid are not uniformly distributed . with a simulation , we have estimated the the number of points in the grid @xmath37 , which is , in the high frequency limit : @xmath38 @xmath39 is an extra resolution factor , which can be greater than 1 , to enhance the efficiency , but even less than 1 , to save computing cost , obviously worsening the efficiency . fig.[fig : gridsim1 ] shows the optimal sky grid , for @xmath40 ( which corresponds to a source frequency @xmath41 hz ) . as already said , the grid used in the sky hough method , is not optimal , but rectangular , to use fastest computing algorithms . the number of points in this rectangular grid is : @xmath42 which is , asymptotically , a factor @xmath43 higher then the number of points of the optimal grid . in fact this grid has to be over resolved to maintain the same sensitivity of the corresponding optimal grid . further , we note that this over resolution produces a higher number of candidates from certain sky positions . , x - axis : ecliptical longitude , degrees , from 0 to 400 ; y - axis : ecliptical latitude , degrees , from -100 to 100 ; the number of points in the map is @xmath37=2902.,width=453 ] the sensitivity of the sky hough procedure is affected by artifacts , i.e. an excess of candidates in some places of the sky map , which are due to local spectral disturbances . the effect ca nt be eliminated because each map is constructed over the whole sky , and hence the threshold for candidate selection has to be the same for the whole sky . using the frequency hough procedure this effect disappears because each map is constructed for only one position in the sky . so , because of the adaptivity of the threshold , if a sky region gives an excess of candidates , the threshold is raised and then there is a loss in sensitivity only for that sky region . we are now ready to enter into details by studying the efficiency of both the methods , by the use of simulations . figure [ fig : gridsim2 ] is an example of how a frequency hough map looks like , having injected into white noise three signals , at different frequencies and spin - down . to study the efficiency of the methods , as a function of the frequency over resolution factor , we have simulated a signal in the absence of noise . the reason for this is that we were interested in studying only the losses due to the discretization errors . the parameters chosen for the simulation are similar to actual situations ( detector parameters , source expected parameters ) . the parameters of the simulation are shown in table [ tab : par ] . [ fig : freqloss ] shows the amplitude loss versus the frequency over resolution factor @xmath45 . the loss was calculated as the average value of all the peaks found in the 500 spectra ( it is important to remember that our procedure considers peaks only the maxima above threshold ) . the result is clear : using @xmath46 the amplitude loss is 3.6 @xmath47 ( the efficiency @xmath48 ) , while with @xmath49 , which is the only practically possible choice of the sky hough , the amplitude loss is 11.6 @xmath47 ( the efficiency @xmath50 ) . from the figure , we notice that there is no further gain of increasing the over resolution factor over 10 . thus , we fixed to 10 the over resolution factor for the frequency hough . in next simulations , results with @xmath46 are thus for the frequency hough , results with @xmath49 are for the sky hough . once we have fixed the frequency over resolution factor we wanted to quantify how the increasing of the spin down resolution from the nominal one would affect the sensitivity . the results are in fig . [ fig : freqloss1 ] , which shows the loss in amplitude vs the spin down over resolution factor , for both the cases @xmath49 , sky hough , and @xmath46,frequency hough . it can be noticed that , in the case of the frequency hough , even for the worst analyzed situation , which corresponds to the nominal spin down step @xmath51 the loss is quite small . is is 3.6 @xmath47 ( the efficiency @xmath48 ) . the situation is worst for the sky hough , where the loss in amplitude at the nominal spin down step is 11.6 @xmath47 ( the efficiency @xmath50 ) . the improvement obtained by a better spin down resolution is not so important , as can be seen from the figure . it seems reasonable , given the observation that increasing the spin down resolution has a computational cost for both the methods , to use the nominal @xmath52 resolution ( x - axis equal to 1 in the figure ) . to study the loss due to the sky grid resolution , we have simulated 50 signals , randomly distributed over the sky . we have then looked for results using the optimal grid , again registering the average value of all the detected peaks . in what follows , we suppose to use the optimal grid for both the procedures , sky and frequency hough . fig.[fig : loss_spinres ] shows the amplitude losses , as a function of the over resolution sky map factor @xmath39 , in the two cases of @xmath46 ( left ) , frequency hough , and @xmath49 ( right ) , sky hough . the amplitude loss , for @xmath44 , is @xmath53 for the frequency hough , and @xmath54 , for the sky hough . again , a better efficiency for the new procedure . we notice that the use of an over resolution for the sky map , would have an impact on the computing cost , with both the procedures . . the figures compare the loss when @xmath46 ( left ) , frequency hough , and when @xmath55 ( right ) , sky hough.,title=""fig:"",width=302 ] . the figures compare the loss when @xmath46 ( left ) , frequency hough , and when @xmath55 ( right ) , sky hough.,title=""fig:"",width=302 ] + we see that the ratio of the amplitude efficiencies is @xmath57 which in power is 1.317 . from this , we can compute the gain in computing cost for the same sensitivity . let us firstly recall that the @xmath58 sensitivity in the hierarchical search is proportional to @xmath59 , and the computing cost to @xmath60 . thus , the `` equivalent fft '' length factor is @xmath61=1.734 and the gain in computing cost is @xmath62=5.2 ( that is , the ratio of computing costs needed to have the same @xmath58 sensitivity ) . * the * adaptivity * , that is the weight of peaks to consider the noise level and the gain due to the antenna pattern toward a direction , is , with this approach , immediate and very simple , as each hough map is done for a single sky position . it has been shown , with the sky hough , that the adaptivity of the procedure is a very important task for the analysis ; * this new procedure is appropriate also for all those situations in which the * source position * is known and we should estimate only source frequency and spin down ; * with a proper choice of parameters , it is also possible to detect and hence remove * spurious signals * , with a constant or linearly varying frequency . on the latter point , we are now working to study the efficiency of this method in terms of rejection of spurious lines in the peakmap . we know that this is a very critical task for the analysis , since the presence of spurious lines highly affects the sensitivity of the search . we expect this new method to be much more insensitive to the presence of spurious lines , since in the chosen hough plane spurious lines and g.w . signals should have a very different and well separable behavior . b. krisnan , a. sintes , m. a. papa , b . f. schutz , s. frasca , c. palomba , _ phys.rev.d70:082001_ , 2004 . `` the hough transform search for continuous gravitational waves '' a. sintes , b. krisnan , _ phys.conf.ser.32:206-211_ , 2006 . `` improved hough search for gravitational wave pulsars '' p. astone , s. frasca , c. palomba,_cqg 22:s1197-s1210_,2005 `` the short fft database and the peakmap for the hierarchical search of periodic sources '' s. frasca , p. astone , c. palomba,_cqg 22:s1013-s1019 _ , 2005 `` evaluation of sensitivity and computing power for the virgo hierarchical search for periodic sources '' c. palomba , p. astone , s. frasca , _ cqg 22:s1255-s1264_,2005 `` adaptive hough transform for the search of periodic sources '' f. acernese et al ( virgo coll . ) _ cqg 24:s491-s499 _ , 2007 `` coincidence analysis between periodic source candidates in c6 and c7 virgo data '' f. acernese et al ( virgo coll . ) _ proceedings of the eleventh marcel grossmann meeting on general relativity ( berlin , 2006 ) edited by h. kleinert , r.t . jantzen and r. ruffini , world scientific , singapore _ , 2008 `` first coincidence search among periodic gravitational wave source candidates using virgo data '' p. astone , s. frasca , c. palomba_proceedings of the eleventh marcel grossmann meeting on general relativity ( berlin 2006 ) edited by h. kleinert , r.t . jantzen and r. ruffini , world scientific , singapore _ , 2008 `` incoherent strategies for the network detection of periodic gravitational waves '' c. palomba , s. frasca , _ cqg 21:s1645-s1654 _ , 2004 `` spectral filtering for hierarchical search of periodic sources ''","in the hierarchical search for periodic sources of gravitational waves , the candidate selection , in the incoherent step , can be performed with hough transform procedures . in this paper we analyze the problem of sensitivity loss due to discretization of the parameters space vs computing cost , comparing the properties of the sky hough procedure with those of a new frequency hough , which is based on a transformation from the _ time - observed frequency _ plane to the _ source frequency - spin down _ plane . results on simulated peakmaps suggest various advantages in favor of the use of the frequency hough . the ones which show up to really make the difference are 1 ) the possibility to enhance the frequency resolution without relevantly affecting the computing cost . this reduces the digitization effects ; 2 ) the excess of candidates due to local disturbances in some places of the sky map . they do not affect the new analysis because each map is constructed for only one position in the sky . + pacs . numbers : 04.80nn,07.05kf,97.60jd"
